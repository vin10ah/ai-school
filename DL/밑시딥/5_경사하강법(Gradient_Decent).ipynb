{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eGZ0TCq_yf5"
      },
      "source": [
        "## 경사하강법(Gradient Decent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixzUUedJVFHF"
      },
      "source": [
        "### 볼록함수(Convex Function)\n",
        "- 어떤 지점에서 시작하더라도 최적값(손실함수가 최소로하는 점)에 도달할 수 있음\n",
        "\n",
        "- 1-D Convex Function\n",
        "![](https://www.researchgate.net/profile/Miodrag_Mateljevic/publication/313821095/figure/fig5/AS:476113622310916@1490525741603/A-strictly-convex-function.png)\n",
        "<br /><sub>출처: https://www.researchgate.net/figure/A-strictly-convex-function_fig5_313821095</sub>\n",
        "\n",
        "- 2-D Convex Function  \n",
        "![](https://www.researchgate.net/publication/275069197/figure/fig8/AS:324418665500689@1454358845613/Sphere-function-D-2.png)\n",
        "<br /><sub>출처: https://www.researchgate.net/figure/Sphere-function-D-2_fig8_275069197</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVo4WnbtRiUM"
      },
      "source": [
        "### 비볼록함수(Non-Convex Function)\n",
        "\n",
        "- 비볼록 함수는 시작점 위치에 따라 다른 최적값에 도달할 수 있음.\n",
        "\n",
        "- 1-D Non-Convex Function\n",
        "![](https://image1.slideserve.com/2659452/example-of-non-convex-function-l.jpg)\n",
        "\n",
        "<sub>출처: https://www.slideserve.com/betha/local-and-global-optima</sub>\n",
        "\n",
        "- 2-D Non-Convex Function\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/e/e3/Non-Convex_Objective_Function.gif)\n",
        "\n",
        "<sub>출처: https://commons.wikimedia.org/wiki/File:Non-Convex_Objective_Function.gif</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1u8PzsUVMBZ"
      },
      "source": [
        "### 경사하강법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMXY9TYKTa4P"
      },
      "source": [
        "#### 미분과 기울기\n",
        "- 스칼라를 벡터로 미분한 것\n",
        "\n",
        "## $\\quad \\frac{df(x)}{dx} = \\lim_{\\triangle x \\to 0} \\frac{f(x+\\triangle x) - f(x)}{\\triangle x}$\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Gradient2.svg/2560px-Gradient2.svg.png)\n",
        "\n",
        "<sub>출처: https://ko.wikipedia.org/wiki/%EA%B8%B0%EC%9A%B8%EA%B8%B0_(%EB%B2%A1%ED%84%B0)</sub>\n",
        "\n",
        "  ## $\\quad \\triangledown f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2},\\ ... \\ , \\frac{\\partial f}{\\partial x_N} \\right)$\n",
        "  - 변화가 있는 지점에서는 미분값이 존재하고, 변화가 없는 지점은 미분값이 0\n",
        "  - 미분값이 클수록 변화량이 크다는 의미\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmZo4rLNTccp"
      },
      "source": [
        "#### 경사하강법의 과정\n",
        "\n",
        "- 경사하강법은 한 스텝마다의 미분값에 따라 이동하는 방향을 결정\n",
        "\n",
        "- $f(x)$의 값이 변하지 않을 때까지 반복\n",
        "\n",
        "  ## $\\qquad x_n = x_{n-1} - \\eta \\frac{\\partial f}{\\partial x}$\n",
        "    \n",
        "    - $\\eta$ : 학습률(learning rate)\n",
        "\n",
        "- 즉, **미분값이 0인 지점**을 찾는 방법  \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/0*fU8XFt-NCMZGAWND.)\n",
        "<br /><sub>출처: https://www.kdnuggets.com/2018/06/intuitive-introduction-gradient-descent.html</sub>\n",
        "\n",
        "\n",
        "- 2-D 경사하강법\n",
        "\n",
        "![](https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif)\n",
        "<br /><sub>출처: https://gfycat.com/ko/angryinconsequentialdiplodocus</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsK-9-3cEphu"
      },
      "source": [
        "#### 경사하강법 구현\n",
        "\n",
        "$\\quad f_1(x) = x^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CUGyiQMEsAD"
      },
      "source": [
        "def f1(x):\n",
        "  return x**2\n",
        "\n",
        "def df_dx1(x):\n",
        "  return 2*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzTrK1gcEr6g"
      },
      "source": [
        "def gradient_descent(f, df_dx, init_x, learning_rate=0.01, step_num=100):\n",
        "  x = init_x\n",
        "  x_log, y_log = [x], [f(x)]\n",
        "\n",
        "  for i in range(step_num):\n",
        "    grad = df_dx(x)\n",
        "    x = x - learning_rate * grad\n",
        "\n",
        "    x_log.append(x)\n",
        "    y_log.append(f(x))\n",
        "\n",
        "  return x_log, y_log\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEicOhyGE48y"
      },
      "source": [
        "#### 경사하강법 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8sIzMtlEr33"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x_init = 5\n",
        "x_log, y_log = gradient_descent(f1, df_dx1, init_x=x_init)\n",
        "plt.scatter(x_log, y_log, c='r')\n",
        "\n",
        "x = np.arange(-5, 5, 0.01)\n",
        "plt.plot(x, f1(x))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVWMFoEFAPJ"
      },
      "source": [
        "#### 비볼록 함수(Non-Convex Function)에서의 경사하강법\n",
        "\n",
        "$\\quad f_2(x) = 0.01x^4 - 0.3x^3 - 1.0x + 10.0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLcnM77vEr2K"
      },
      "source": [
        "def f2(x):\n",
        "  return 0.01*x**4 - 0.3*x**3 - 1.0*x + 10.0\n",
        "\n",
        "def df_dx2(x):\n",
        "  return 0.04*x**3 - 0.9*x**2 - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPCibdNFFLzm"
      },
      "source": [
        "#### 비볼록함수 경사하강법 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IJDWUno9Tpu"
      },
      "source": [
        "x_init = 2\n",
        "x_log, y_log = gradient_descent(f2, df_dx2, init_x=x_init)\n",
        "\n",
        "plt.scatter(x_log, y_log, c='r')\n",
        "\n",
        "x = np.arange(-5, 30, 0.01)\n",
        "plt.plot(x, f2(x))\n",
        "plt.xlim(-5, 30)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrOS8DCFUdK"
      },
      "source": [
        "### 전역 최적값 vs 지역 최적값\n",
        "- 초기값이 어니냐에 따라 전체 함수의 최솟값이 될 수도 있고,  \n",
        "  지역적으로 최솟값일 수 있음\n",
        "\n",
        "![](https://www.kdnuggets.com/wp-content/uploads/function-max-global.jpg)\n",
        "<br /><sub>출처: https://www.kdnuggets.com/2017/06/deep-learning-local-minimum.html</sub>\n",
        "\n",
        "$\\quad f_3(x) = x sin(x^2) + 1$ 그래프"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv3mWccFEryk"
      },
      "source": [
        "def f3(x):\n",
        "  return x*np.sin(x**2) + 1\n",
        "\n",
        "def df_dx3(x):\n",
        "  return np.sin(x**2) + 2*x*np.cos(x**2)*2*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR80FK0bGba-"
      },
      "source": [
        "#### 전역 최솟값 vs 지역 최솟값 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR2S2zp8Erty"
      },
      "source": [
        "x_init1 = -0.5\n",
        "x_log1, y_log1 = gradient_descent(f3, df_dx3, init_x=x_init1)\n",
        "plt.scatter(x_log1, y_log1, c='r')\n",
        "\n",
        "x_init2 = 1.5\n",
        "x_log2, y_log2 = gradient_descent(f3, df_dx3, init_x=x_init2)\n",
        "plt.scatter(x_log2, y_log2, c='b')\n",
        "\n",
        "x = np.arange(-3, 3, 0.01)\n",
        "plt.plot(x, f3(x), '--')\n",
        "\n",
        "plt.scatter(x_init1, f3(x_init1), c='r')\n",
        "plt.text(x_init1-1.0, f3(x_init1)+0.3, 'x_init1 ({})'.format(x_init1), fontsize=13)\n",
        "\n",
        "plt.scatter(x_init2, f3(x_init2), c='b')\n",
        "plt.text(x_init2-0.7, f3(x_init2)+0.4, 'x_init2 ({})'.format(x_init2), fontsize=13)\n",
        "\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DsArF9fHpwA"
      },
      "source": [
        "#### 경사하강법 구현(2)\n",
        "- 경사하강을 진행하는 도중, 최솟값에 이르면 경사하강법을 종료하는 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1cgn4Gr_uJI"
      },
      "source": [
        "def gradient_descent2(f, df_dx, init_x, learning_rate=0.01, step_num=100):\n",
        "  eps = 1e-5\n",
        "  count = 0\n",
        "\n",
        "  old_x = init_x\n",
        "  min_x = old_x\n",
        "  min_y = f(min_x)\n",
        "\n",
        "  x_log, y_log = [min_x], [min_y]\n",
        "\n",
        "  for i in range(step_num):\n",
        "    grad = df_dx(old_x)\n",
        "    new_x = old_x - learning_rate * grad\n",
        "    new_y = f(new_x)\n",
        "\n",
        "    if min_y > new_y:\n",
        "      min_x = new_x\n",
        "      min_y = new_y\n",
        "\n",
        "    if np.abs(new_x - old_x) < eps:\n",
        "      break\n",
        "\n",
        "    x_log.append(old_x)\n",
        "    y_log.append(new_y)\n",
        "\n",
        "    old_x = new_x\n",
        "    count +=1\n",
        "\n",
        "  return x_log, y_log, count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZWU40x2Mc8X"
      },
      "source": [
        "$f_3(x) = x sin(x^2) + 1$ 그래프\n",
        "- 각 시작점마다 경사하강법으로 내려가다가 최솟값으로 인지하는 부분에서 멈춤\n",
        "\n",
        "  step_num(반복횟수)만큼 다 돌지 않는 경우도 발생  \n",
        "\n",
        "  하지만 주어진 범위 내에서의 최솟값은 첫번째 시작점일 때이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUJk31aXIH_p"
      },
      "source": [
        "x_init1 = -2.2\n",
        "x_log1, y_log1, count1 = gradient_descent2(f3, df_dx3, init_x=x_init1)\n",
        "plt.scatter(x_log1, y_log1, c='r')\n",
        "print('count1: {}'.format(count1))\n",
        "\n",
        "x_init2 = -0.5\n",
        "x_log2, y_log2, count2 = gradient_descent2(f3, df_dx3, init_x=x_init2)\n",
        "plt.scatter(x_log2, y_log2, c='b')\n",
        "print('count2: {}'.format(count2))\n",
        "\n",
        "x_init3 = 1.5\n",
        "x_log3, y_log3, count3 = gradient_descent2(f3, df_dx3, init_x=x_init3)\n",
        "plt.scatter(x_log3, y_log3, c='g')\n",
        "print('count3: {}'.format(count3))\n",
        "\n",
        "x = np.arange(-3, 3, 0.01)\n",
        "plt.plot(x, f3(x), '--')\n",
        "\n",
        "plt.scatter(x_init1, f3(x_init1), c='r')\n",
        "plt.text(x_init1+0.2, f3(x_init1)+0.2, 'x_init1 ({})'.format(x_init1), fontsize=13)\n",
        "\n",
        "plt.scatter(x_init2, f3(x_init2), c='b')\n",
        "plt.text(x_init2+0.1, f3(x_init2)-0.3, 'x_init2 ({})'.format(x_init2), fontsize=13)\n",
        "\n",
        "plt.scatter(x_init3, f3(x_init3), c='g')\n",
        "plt.text(x_init3-1.0, f3(x_init3)+0.3, 'x_init3 ({})'.format(x_init3), fontsize=13)\n",
        "\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd2g3OL-Efwv"
      },
      "source": [
        "### 학습률(learning rate)\n",
        "- 학습률 값은 적절히 지정해야 한다!\n",
        "- 너무 크면 발산하고, 너무 작으면 학습이 잘 되지 않는다.\n",
        "  \n",
        "![](https://cdn-images-1.medium.com/freeze/max/1000/1*22oh44C5tUHbZ0yvIKWDFg.png)\n",
        "<sub>출처: https://mc.ai/an-introduction-to-gradient-descent-algorithm/</sub>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwi3xvcdEg-f"
      },
      "source": [
        "x_init = 10\n",
        "x_log, y_log, _ = gradient_descent2(f1, df_dx1, init_x=x_init, learning_rate=1.05)\n",
        "plt.plot(x_log, y_log, c='r')\n",
        "\n",
        "plt.scatter(x_init, f1(x_init), c='b')\n",
        "plt.text(x_init-2.2, f1(x_init)-2, 'x_init ({})'.format(x_init), fontsize=10)\n",
        "\n",
        "x = np.arange(-50, 30, 0.01)\n",
        "plt.plot(x, f1(x), '--')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXFkHxVvTX8T"
      },
      "source": [
        "#### 학습률별 경사하강법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhdS_6RmTXTu"
      },
      "source": [
        "lr_list = [0.001, 0.01, 0.1, 1.01]\n",
        "\n",
        "init_x = 30.0\n",
        "x = np.arange(-30, 50, 0.01)\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "\n",
        "for i, lr in enumerate(lr_list):\n",
        "  x_log, y_log, count = gradient_descent2(f1, df_dx1, init_x=init_x, learning_rate=lr)\n",
        "  ax = fig.add_subplot(2, 2, i+1)\n",
        "  ax.scatter(init_x, f1(init_x), c='g')\n",
        "  ax.plot(x_log, y_log, color='r', linewidth='4')\n",
        "  ax.plot(x, f1(x), '--')\n",
        "  ax.grid()\n",
        "  ax.title.set_text('learning rate = {}'.format(str(lr)))\n",
        "  print('init value = {}, count = {}'.format(str(lr), str(count)))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMFZIwN7Ehty"
      },
      "source": [
        "### 안장점(Saddle Point)\n",
        "\n",
        "- 기울기가 0이지만 극값이 되지 않음\n",
        "- 경사하강법은 안장점에서 벗어나지 못함\n",
        "\n",
        "![](https://e7.pngegg.com/pngimages/413/127/png-clipart-saddle-point-graph-of-a-function-gradient-descent-deep-learning-mathematics-mathematics-angle-furniture-thumbnail.png)\n",
        "<br /><sub>출처: https://www.pngegg.com/en/png-czdxs</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRsj9SgHIG_1"
      },
      "source": [
        "$f_2(x) = 0.01x^4 - 0.3x^3 - 1.0x + 10.0$ 그래프로 확인하기\n",
        "\n",
        "- 첫번째 시작점  \n",
        "  - count가 100, 즉 step_num(반복횟수)만큼 루프를 돌았음에도  \n",
        "  손실함수의 값이 10 언저리에서 멈춤. 변화 X\n",
        "  - 안장점 (Saddle Point)\n",
        "  \n",
        "  - 이는 학습률(learning rate)조절 또는 다른 초기값 설정을 통해 수정해야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML3HRPB2IHeF"
      },
      "source": [
        "x_init1 = -10.0\n",
        "x_log1, y_log1, count1 = gradient_descent2(f2, df_dx2, init_x=x_init1)\n",
        "plt.scatter(x_log1, y_log1, c='r')\n",
        "print('count1: {}'.format(count1))\n",
        "\n",
        "x_init2 = 5.0\n",
        "x_log2, y_log2, count2 = gradient_descent2(f2, df_dx2, init_x=x_init2)\n",
        "plt.scatter(x_log2, y_log2, c='b')\n",
        "print('count2: {}'.format(count2))\n",
        "\n",
        "x_init3 = 33.0\n",
        "x_log3, y_log3, count3 = gradient_descent2(f2, df_dx2, init_x=x_init3)\n",
        "plt.scatter(x_log3, y_log3, c='g')\n",
        "print('count3: {}'.format(count3))\n",
        "\n",
        "x = np.arange(-15, 35, 0.01)\n",
        "plt.plot(x, f2(x), '--')\n",
        "\n",
        "plt.scatter(x_init1, f2(x_init1), c='r')\n",
        "plt.text(x_init1+2, f2(x_init1), 'x_init1 ({})'.format(x_init1), fontsize=13)\n",
        "\n",
        "plt.scatter(x_init2, f2(x_init2), c='b')\n",
        "plt.text(x_init2+2, f2(x_init2), 'x_init2 ({})'.format(x_init2), fontsize=13)\n",
        "\n",
        "plt.scatter(x_init3, f2(x_init3), c='g')\n",
        "plt.text(x_init3-10, f2(x_init3), 'x_init3 ({})'.format(x_init3), fontsize=13)\n",
        "\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zUn7KsURUI"
      },
      "source": [
        "$f_3(x) = x sin(x^2) + 1$ 그래프에서 확인하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ExtoNteEiFx"
      },
      "source": [
        "x_init1 = -2.2\n",
        "x_log1, y_log1, count1 = gradient_descent2(f3, df_dx3, init_x=x_init1)\n",
        "plt.scatter(x_log1, y_log1, c='r')\n",
        "print('count1: {}'.format(count1))\n",
        "\n",
        "x_init2 = 1.2\n",
        "x_log2, y_log2, count2 = gradient_descent2(f3, df_dx3, init_x=x_init2)\n",
        "plt.scatter(x_log2, y_log2, c='b')\n",
        "print('count2: {}'.format(count2))\n",
        "\n",
        "x = np.arange(-3, 3, 0.01)\n",
        "plt.plot(x, f3(x), '--')\n",
        "\n",
        "plt.scatter(x_init1, f3(x_init1), c='r')\n",
        "plt.text(x_init1+0.2, f3(x_init1)+0.2, 'x_init1 ({})'.format(x_init1), fontsize=13)\n",
        "\n",
        "plt.scatter(x_init2, f3(x_init2), c='b')\n",
        "plt.text(x_init2-1.0, f3(x_init2)+0.3, 'x_init2 ({})'.format(x_init2), fontsize=13)\n",
        "\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}