{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFqiiMDKt-_R"
      },
      "source": [
        "# ìì—°ì–´ ì²˜ë¦¬ ê°ì •ë¶„ì„ ì „ì´í•™ìŠµ\n",
        "\n",
        "- GPUë¡œ ëŸ°íƒ€ì„ ì—°ê²°í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2-prJQyytlFu"
      },
      "outputs": [],
      "source": [
        "!pip install -qq torch transformers datasets numpy evaluate pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eJry2QDMxHcq"
      },
      "outputs": [],
      "source": [
        "!pip install -qq accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LnRYcFqxuEt8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vin_ah/miniconda3/envs/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O3bvK54_uaec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 36000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 1333\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 2667\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"sepidmnorozy/Korean_sentiment\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b3jgyKeXvgq6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'label': 1, 'text': 'ì¡¸ì¼!!!ì„±ì•„ê°€ë‚˜ì¤‘ì—ì–µìš¸í•œì¼ì´ì‡ì–´ì„œì¢€ìŠ¬íëŠ”ë°ë§ˆì§€ë§‰ì€ê¸°ì˜ê²Œëë‚˜ì„œë‹¤í–‰ì´ì—ì—¬'}\n",
            "{'label': 0, 'text': 'ì§„ì§œ ì–´ë–»ê²Œ ëœë†ˆì˜ ì˜í™”ê°€ ì—¬ê³ ê´´ë‹´ 1ë³´ë‹¤ë„ ëª»í•¨? ì‹ ê¸°í•˜ë‹¤ ê·¸ê²ƒë„ 2012ë…„ì‘ì´ 1998ë…„ë³´ë‹¤ ëª»í•¨ ì†”ê¹Œ ì—¬ê³ ê´´ë‹´1ì€ ë°˜ì „ì€ ìµœê³ ì§€ ë­ ì´ë†ˆì˜ ì˜í™”ëŠ” ì—¬ê³ ê´´ë‹´ ì‹œë¦¬ì¦ˆë³´ë‹¤ë„ ëª»í•˜ëŠ”ê±°ê°™ë‹¤'}\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'][3118])\n",
        "print(dataset['train'][14310])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqadEZqBvoS6"
      },
      "source": [
        "## í† í°í™” Tokenize\n",
        "\n",
        "https://huggingface.co/kykim/bert-kor-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AbCIxx1zv6oh"
      },
      "outputs": [],
      "source": [
        "model_name = \"kykim/bert-kor-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Dtyaj7_SwA1Q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vin_ah/miniconda3/envs/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='kykim/bert-kor-base', vocab_size=42000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fzx41j-KwDNh"
      },
      "outputs": [],
      "source": [
        "def tokenizer_func(x):\n",
        "    return tokenizer(\n",
        "        x['text'],\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        truncation=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jTaKS9LswJfJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1333/1333 [00:00<00:00, 4672.60 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = dataset.map(tokenizer_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZNP5brZ7wMYH"
      },
      "outputs": [],
      "source": [
        "train_num_samples = 10000\n",
        "\n",
        "train_ds = tokenized_datasets['train'].shuffle(seed=42).select(range(train_num_samples))\n",
        "eval_ds = tokenized_datasets['validation'].shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JlT3K8AwWKU"
      },
      "source": [
        "## ì „ì´í•™ìŠµ Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JPMzAZUcwYw9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCns6wNcwaqx"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MqoSboLswmK4"
      },
      "outputs": [],
      "source": [
        "bs = 16\n",
        "epochs = 4\n",
        "lr = 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN82EavJ0Nof"
      },
      "source": [
        "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CUieJ5m4wnpt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vin_ah/miniconda3/envs/new_env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "args = TrainingArguments(\n",
        "    'outputs',\n",
        "    learning_rate=lr,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type='cosine',\n",
        "    bf16=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    gradient_accumulation_steps=4, # until bs=128\n",
        "    eval_accumulation_steps=4,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.01,\n",
        "    report_to='none'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UJ2UuJ-wq2f"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fIJu9s95xd55"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load('accuracy')\n",
        "\n",
        "# all Transformers models return logits\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=preds, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rTC7HUbxg6g"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u55sBP23xjJX"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L-QuqHpfxkSq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/624 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAh3r4Mc1znn"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"./mymodel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahPYMKSvxppw"
      },
      "source": [
        "## ì¶”ë¡  Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh8ZDutw3iL4"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline('text-classification', model=\"./mymodel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqijAkon24W7"
      },
      "source": [
        "### í…ŒìŠ¤íŠ¸ì…‹ ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSCVgA5f26Rd"
      },
      "outputs": [],
      "source": [
        "test_data = dataset['validation'].shuffle(seed=424)[:100]\n",
        "td = pd.DataFrame(test_data)\n",
        "td"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV4p8rAZ27r2"
      },
      "outputs": [],
      "source": [
        "preds = pipe(td['text'].tolist())\n",
        "\n",
        "preds_df = pd.DataFrame(preds)\n",
        "preds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qygzprf_32lg"
      },
      "outputs": [],
      "source": [
        "preds_df.rename(columns={'label':'pred'}, inplace=True)\n",
        "preds_df['pred'] = preds_df['pred'].map({'LABEL_1': 1, 'LABEL_0': 0})\n",
        "\n",
        "preds_df = pd.concat([preds_df, td], axis=1)\n",
        "preds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYLRc9IN4ATl"
      },
      "outputs": [],
      "source": [
        "mask = preds_df['pred'] == preds_df['label']\n",
        "\n",
        "len(preds_df[mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKSFksiy4WlQ"
      },
      "source": [
        "### ë‚´ ë°ì´í„°ì…‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDfEFjA-yk9-"
      },
      "outputs": [],
      "source": [
        "txts = [\n",
        "    {'label': 0, 'text': \"ì ˆëŒ€ë¡œ ê°•ì¶”í•  ìˆ˜ ì—†ëŠ” ì˜í™”\"},\n",
        "    {'label': 0, 'text': \"ì ˆëŒ€ë¡œ ì¶”ì²œí•  ìˆ˜ ì—†ëŠ” ì˜í™”\"},\n",
        "    {'label': 1, 'text': \"ë˜ ë³´ê³  ì‹¶ë‹¤.\"},\n",
        "    {'label': 0, 'text': \"ì´ê±¸ ë³´ë©´ì„œ ì›ƒì„ ìˆ˜ëŠ” ì—†ë‹¤.\"},\n",
        "    {'label': 0, 'text': \"ì²˜ìŒì—ëŠ” ì¬ë¯¸ìˆì—ˆëŠ”ë° ê°ˆìˆ˜ë¡ ì‚°ìœ¼ë¡œ ê°€ëŠ” ë‚´ìš©.\"},\n",
        "    {'label': 1, 'text': \"ìš”ì¦˜ ì¬ë¯¸ì—†ëŠ” ì˜í™”ë§Œ ë‚˜ì˜¤ëŠ”ë° ì‹ ì„ í•œ ì¶©ê²©ì„ ì¤€ ì˜í™”.\"},\n",
        "    {'label': 1, 'text': \"ìœ ëª…í•œ ê°ë…ì´ë‚˜ ë°°ìš°ê°€ ë‚˜ì˜¤ì§€ëŠ” ì•Šì§€ë§Œ ìŠ¤í† ë¦¬ê°€ ê°ë™\"}\n",
        "]\n",
        "\n",
        "txts_td = pd.DataFrame(txts)\n",
        "txts_td"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUTJhNJ34UyS"
      },
      "outputs": [],
      "source": [
        "preds_txts = pipe(txts_td['text'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcBD2dO04c6p"
      },
      "outputs": [],
      "source": [
        "preds_txts_df = pd.DataFrame(preds_txts)\n",
        "preds_txts_df.rename(columns={'label':'pred'}, inplace=True)\n",
        "preds_txts_df['pred'] = preds_txts_df['pred'].map({'LABEL_1': 1, 'LABEL_0': 0})\n",
        "\n",
        "preds_txts_df = pd.concat([preds_txts_df, txts_td], axis=1)\n",
        "preds_txts_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-193EjyD4phm"
      },
      "source": [
        "### íŒŒì´í”„ë¼ì¸ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëª¨ë¸ ë¡œë”©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DbSC0xmytOg"
      },
      "outputs": [],
      "source": [
        "model_inf = AutoModelForSequenceClassification.from_pretrained(\"./mymodel\")\n",
        "inputs = tokenizer(txts_td['text'].tolist(), padding=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model_inf(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11TGxh3Q2UZI"
      },
      "outputs": [],
      "source": [
        "logits.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX2ycOTAzALK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
