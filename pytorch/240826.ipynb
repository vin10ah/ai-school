{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "# torch 버전 확인\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu 사용 가능 여부 확인\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 3., 4., 5., 6.,])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([6])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim()) # rank = 차원\n",
    "print(t.shape) # shape\n",
    "print(t.size()) # shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([3., 4., 5.]) tensor([5.])\n",
      "tensor([0., 1.]) tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(t[0], t[1], t[-1]) # 인덱스로 접근\n",
    "print(t[2:5], t[4:-1]) # 슬라이싱\n",
    "print(t[:2], t[3:]) # 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "print(t.max()) # Returns one value: max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0)) # Returns two values: max and argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zebra'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10개 사물 분류 모델\n",
    "result = torch.FloatTensor([0.1, 0.04, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.89])\n",
    "names = ['cat', 'dog', 'lion', 'tiger', 'wolf', 'fox', 'rabbit', 'snake', 'zebra',  'horse']\n",
    "names[result.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "ft.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([3, 4])\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([4, 3])\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([2, -1])\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([4, 3, 1])\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqeeze & Unsqeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.],\n",
      "         [1.],\n",
      "         [2.]]])\n",
      "torch.Size([1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
    "print(ft.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad=True) # requires_grad = True로 설정해줘야 가중치가 바뀜, 학습이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x_train * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean((y_train - y_pred) ** 2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([W, b], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # 다음 계산을 위한 가중치 초기화\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step() # 파라미터 값[W, b] 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/30 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch    1/30 W: 0.353, b: 0.151 Cost: 14.770963\n",
      "Epoch    2/30 W: 0.500, b: 0.214 Cost: 11.691541\n",
      "Epoch    3/30 W: 0.632, b: 0.270 Cost: 9.257346\n",
      "Epoch    4/30 W: 0.749, b: 0.319 Cost: 7.333169\n",
      "Epoch    5/30 W: 0.853, b: 0.363 Cost: 5.812135\n",
      "Epoch    6/30 W: 0.945, b: 0.401 Cost: 4.609764\n",
      "Epoch    7/30 W: 1.028, b: 0.435 Cost: 3.659278\n",
      "Epoch    8/30 W: 1.101, b: 0.466 Cost: 2.907896\n",
      "Epoch    9/30 W: 1.166, b: 0.492 Cost: 2.313895\n",
      "Epoch   10/30 W: 1.224, b: 0.516 Cost: 1.844294\n",
      "Epoch   11/30 W: 1.276, b: 0.536 Cost: 1.473027\n",
      "Epoch   12/30 W: 1.322, b: 0.555 Cost: 1.179487\n",
      "Epoch   13/30 W: 1.363, b: 0.571 Cost: 0.947386\n",
      "Epoch   14/30 W: 1.400, b: 0.585 Cost: 0.763851\n",
      "Epoch   15/30 W: 1.433, b: 0.597 Cost: 0.618704\n",
      "Epoch   16/30 W: 1.462, b: 0.608 Cost: 0.503902\n",
      "Epoch   17/30 W: 1.488, b: 0.617 Cost: 0.413086\n",
      "Epoch   18/30 W: 1.511, b: 0.625 Cost: 0.341229\n",
      "Epoch   19/30 W: 1.531, b: 0.632 Cost: 0.284360\n",
      "Epoch   20/30 W: 1.550, b: 0.638 Cost: 0.239337\n",
      "Epoch   21/30 W: 1.566, b: 0.644 Cost: 0.203679\n",
      "Epoch   22/30 W: 1.581, b: 0.648 Cost: 0.175424\n",
      "Epoch   23/30 W: 1.594, b: 0.652 Cost: 0.153021\n",
      "Epoch   24/30 W: 1.606, b: 0.655 Cost: 0.135243\n",
      "Epoch   25/30 W: 1.617, b: 0.658 Cost: 0.121122\n",
      "Epoch   26/30 W: 1.626, b: 0.660 Cost: 0.109892\n",
      "Epoch   27/30 W: 1.635, b: 0.662 Cost: 0.100947\n",
      "Epoch   28/30 W: 1.642, b: 0.663 Cost: 0.093809\n",
      "Epoch   29/30 W: 1.649, b: 0.664 Cost: 0.088100\n",
      "Epoch   30/30 W: 1.655, b: 0.665 Cost: 0.083519\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 30 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "\t# H(x) 계산\n",
    "\thypothesis = x_train * W + b\n",
    "\n",
    "\t# cost 계산\n",
    "\tcost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "\t# cost로 H(x) 개선\n",
    "\toptimizer.zero_grad()\n",
    "\tcost.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\t# 100번마다 로그 출력\n",
    "\t# if epoch % 100 == 0:\n",
    "\tprint('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "\t\tepoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "\t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module과 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1187d4e70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "\t\t\t\t\t\t\t\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2710], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Linear(3, 1) # parameter = input * output + 1(bias)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/30 Cost: 31667.597656\n",
      "Epoch    1/30 Cost: 9926.265625\n",
      "Epoch    2/30 Cost: 3111.513184\n",
      "Epoch    3/30 Cost: 975.451477\n",
      "Epoch    4/30 Cost: 305.908539\n",
      "Epoch    5/30 Cost: 96.042496\n",
      "Epoch    6/30 Cost: 30.260748\n",
      "Epoch    7/30 Cost: 9.641701\n",
      "Epoch    8/30 Cost: 3.178671\n",
      "Epoch    9/30 Cost: 1.152871\n",
      "Epoch   10/30 Cost: 0.517863\n",
      "Epoch   11/30 Cost: 0.318801\n",
      "Epoch   12/30 Cost: 0.256388\n",
      "Epoch   13/30 Cost: 0.236821\n",
      "Epoch   14/30 Cost: 0.230660\n",
      "Epoch   15/30 Cost: 0.228719\n",
      "Epoch   16/30 Cost: 0.228095\n",
      "Epoch   17/30 Cost: 0.227880\n",
      "Epoch   18/30 Cost: 0.227799\n",
      "Epoch   19/30 Cost: 0.227762\n",
      "Epoch   20/30 Cost: 0.227732\n",
      "Epoch   21/30 Cost: 0.227710\n",
      "Epoch   22/30 Cost: 0.227680\n",
      "Epoch   23/30 Cost: 0.227666\n",
      "Epoch   24/30 Cost: 0.227646\n",
      "Epoch   25/30 Cost: 0.227620\n",
      "Epoch   26/30 Cost: 0.227597\n",
      "Epoch   27/30 Cost: 0.227574\n",
      "Epoch   28/30 Cost: 0.227553\n",
      "Epoch   29/30 Cost: 0.227529\n",
      "Epoch   30/30 Cost: 0.227505\n",
      "Epoch   31/30 Cost: 0.227486\n",
      "Epoch   32/30 Cost: 0.227468\n",
      "Epoch   33/30 Cost: 0.227446\n",
      "Epoch   34/30 Cost: 0.227423\n",
      "Epoch   35/30 Cost: 0.227394\n",
      "Epoch   36/30 Cost: 0.227376\n",
      "Epoch   37/30 Cost: 0.227354\n",
      "Epoch   38/30 Cost: 0.227329\n",
      "Epoch   39/30 Cost: 0.227310\n",
      "Epoch   40/30 Cost: 0.227287\n",
      "Epoch   41/30 Cost: 0.227267\n",
      "Epoch   42/30 Cost: 0.227250\n",
      "Epoch   43/30 Cost: 0.227220\n",
      "Epoch   44/30 Cost: 0.227202\n",
      "Epoch   45/30 Cost: 0.227179\n",
      "Epoch   46/30 Cost: 0.227152\n",
      "Epoch   47/30 Cost: 0.227139\n",
      "Epoch   48/30 Cost: 0.227116\n",
      "Epoch   49/30 Cost: 0.227092\n",
      "Epoch   50/30 Cost: 0.227072\n",
      "Epoch   51/30 Cost: 0.227050\n",
      "Epoch   52/30 Cost: 0.227030\n",
      "Epoch   53/30 Cost: 0.226998\n",
      "Epoch   54/30 Cost: 0.226979\n",
      "Epoch   55/30 Cost: 0.226955\n",
      "Epoch   56/30 Cost: 0.226940\n",
      "Epoch   57/30 Cost: 0.226911\n",
      "Epoch   58/30 Cost: 0.226898\n",
      "Epoch   59/30 Cost: 0.226869\n",
      "Epoch   60/30 Cost: 0.226848\n",
      "Epoch   61/30 Cost: 0.226826\n",
      "Epoch   62/30 Cost: 0.226807\n",
      "Epoch   63/30 Cost: 0.226788\n",
      "Epoch   64/30 Cost: 0.226763\n",
      "Epoch   65/30 Cost: 0.226742\n",
      "Epoch   66/30 Cost: 0.226720\n",
      "Epoch   67/30 Cost: 0.226701\n",
      "Epoch   68/30 Cost: 0.226677\n",
      "Epoch   69/30 Cost: 0.226652\n",
      "Epoch   70/30 Cost: 0.226633\n",
      "Epoch   71/30 Cost: 0.226613\n",
      "Epoch   72/30 Cost: 0.226595\n",
      "Epoch   73/30 Cost: 0.226566\n",
      "Epoch   74/30 Cost: 0.226548\n",
      "Epoch   75/30 Cost: 0.226524\n",
      "Epoch   76/30 Cost: 0.226508\n",
      "Epoch   77/30 Cost: 0.226483\n",
      "Epoch   78/30 Cost: 0.226457\n",
      "Epoch   79/30 Cost: 0.226442\n",
      "Epoch   80/30 Cost: 0.226421\n",
      "Epoch   81/30 Cost: 0.226398\n",
      "Epoch   82/30 Cost: 0.226374\n",
      "Epoch   83/30 Cost: 0.226350\n",
      "Epoch   84/30 Cost: 0.226331\n",
      "Epoch   85/30 Cost: 0.226314\n",
      "Epoch   86/30 Cost: 0.226285\n",
      "Epoch   87/30 Cost: 0.226268\n",
      "Epoch   88/30 Cost: 0.226250\n",
      "Epoch   89/30 Cost: 0.226223\n",
      "Epoch   90/30 Cost: 0.226207\n",
      "Epoch   91/30 Cost: 0.226178\n",
      "Epoch   92/30 Cost: 0.226160\n",
      "Epoch   93/30 Cost: 0.226139\n",
      "Epoch   94/30 Cost: 0.226120\n",
      "Epoch   95/30 Cost: 0.226099\n",
      "Epoch   96/30 Cost: 0.226073\n",
      "Epoch   97/30 Cost: 0.226056\n",
      "Epoch   98/30 Cost: 0.226030\n",
      "Epoch   99/30 Cost: 0.226011\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\t# H(x) 계산\n",
    "\tprediction = model(x_train)\n",
    "\t# model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "\t# cost 계산\n",
    "\tcost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "\t# cost로 H(x) 개선하는 부분\n",
    "\t# gradient를 0으로 초기화\n",
    "\toptimizer.zero_grad()\n",
    "\t# 비용 함수를 미분하여 gradient 계산\n",
    "\tcost.backward()\n",
    "\t# W와 b를 업데이트\n",
    "\toptimizer.step()\n",
    "\n",
    "\t# if epoch % 100 == 0:\n",
    "\t# # 100번마다 로그 출력\n",
    "\tprint('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "\t\tepoch, nb_epochs, cost.item()\n",
    "\t  ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subclass 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear1 = nn.Linear(3, 6)\n",
    "\t\tself.linear2 = nn.Linear(6, 3)\n",
    "\t\tself.linear3 = nn.Linear(3, 1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\tx = self.linear2(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\tx = self.linear3(x)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/30 Cost: 27123.453125\n",
      "Epoch    1/30 Cost: 22235.480469\n",
      "Epoch    2/30 Cost: 15216.559570\n",
      "Epoch    3/30 Cost: 7072.575684\n",
      "Epoch    4/30 Cost: 763.970581\n",
      "Epoch    5/30 Cost: 2062.238281\n",
      "Epoch    6/30 Cost: 7513.181152\n",
      "Epoch    7/30 Cost: 5887.244141\n",
      "Epoch    8/30 Cost: 2085.907471\n",
      "Epoch    9/30 Cost: 113.675697\n",
      "Epoch   10/30 Cost: 371.405853\n",
      "Epoch   11/30 Cost: 1611.809814\n",
      "Epoch   12/30 Cost: 2725.224609\n",
      "Epoch   13/30 Cost: 3211.925781\n",
      "Epoch   14/30 Cost: 2995.195801\n",
      "Epoch   15/30 Cost: 2225.776611\n",
      "Epoch   16/30 Cost: 1201.609009\n",
      "Epoch   17/30 Cost: 326.914398\n",
      "Epoch   18/30 Cost: 2.944823\n",
      "Epoch   19/30 Cost: 377.446503\n",
      "Epoch   20/30 Cost: 1091.085938\n",
      "Epoch   21/30 Cost: 1468.674316\n",
      "Epoch   22/30 Cost: 1189.778809\n",
      "Epoch   23/30 Cost: 563.700623\n",
      "Epoch   24/30 Cost: 94.858597\n",
      "Epoch   25/30 Cost: 20.146202\n",
      "Epoch   26/30 Cost: 245.359833\n",
      "Epoch   27/30 Cost: 535.448242\n",
      "Epoch   28/30 Cost: 696.227966\n",
      "Epoch   29/30 Cost: 650.551880\n",
      "Epoch   30/30 Cost: 438.180267\n",
      "Epoch   31/30 Cost: 179.951569\n",
      "Epoch   32/30 Cost: 17.673609\n",
      "Epoch   33/30 Cost: 33.586174\n",
      "Epoch   34/30 Cost: 185.051910\n",
      "Epoch   35/30 Cost: 325.481140\n",
      "Epoch   36/30 Cost: 329.225739\n",
      "Epoch   37/30 Cost: 203.084991\n",
      "Epoch   38/30 Cost: 60.200634\n",
      "Epoch   39/30 Cost: 1.943473\n",
      "Epoch   40/30 Cost: 42.157822\n",
      "Epoch   41/30 Cost: 122.792625\n",
      "Epoch   42/30 Cost: 173.720001\n",
      "Epoch   43/30 Cost: 160.121429\n",
      "Epoch   44/30 Cost: 96.109756\n",
      "Epoch   45/30 Cost: 28.343296\n",
      "Epoch   46/30 Cost: 1.887618\n",
      "Epoch   47/30 Cost: 27.133417\n",
      "Epoch   48/30 Cost: 72.204048\n",
      "Epoch   49/30 Cost: 92.088898\n",
      "Epoch   50/30 Cost: 69.973419\n",
      "Epoch   51/30 Cost: 28.679865\n",
      "Epoch   52/30 Cost: 3.336978\n",
      "Epoch   53/30 Cost: 8.629827\n",
      "Epoch   54/30 Cost: 31.521042\n",
      "Epoch   55/30 Cost: 48.116814\n",
      "Epoch   56/30 Cost: 44.397781\n",
      "Epoch   57/30 Cost: 24.929064\n",
      "Epoch   58/30 Cost: 6.361625\n",
      "Epoch   59/30 Cost: 2.537759\n",
      "Epoch   60/30 Cost: 13.056242\n",
      "Epoch   61/30 Cost: 24.948605\n",
      "Epoch   62/30 Cost: 25.928766\n",
      "Epoch   63/30 Cost: 15.825595\n",
      "Epoch   64/30 Cost: 4.824893\n",
      "Epoch   65/30 Cost: 2.093392\n",
      "Epoch   66/30 Cost: 7.626948\n",
      "Epoch   67/30 Cost: 14.159554\n",
      "Epoch   68/30 Cost: 15.010895\n",
      "Epoch   69/30 Cost: 9.794062\n",
      "Epoch   70/30 Cost: 3.680647\n",
      "Epoch   71/30 Cost: 1.918213\n",
      "Epoch   72/30 Cost: 4.962806\n",
      "Epoch   73/30 Cost: 8.658819\n",
      "Epoch   74/30 Cost: 8.905089\n",
      "Epoch   75/30 Cost: 5.694978\n",
      "Epoch   76/30 Cost: 2.449535\n",
      "Epoch   77/30 Cost: 2.025415\n",
      "Epoch   78/30 Cost: 4.028217\n",
      "Epoch   79/30 Cost: 5.818024\n",
      "Epoch   80/30 Cost: 5.470122\n",
      "Epoch   81/30 Cost: 3.496747\n",
      "Epoch   82/30 Cost: 1.938640\n",
      "Epoch   83/30 Cost: 2.109483\n",
      "Epoch   84/30 Cost: 3.382361\n",
      "Epoch   85/30 Cost: 4.119272\n",
      "Epoch   86/30 Cost: 3.523310\n",
      "Epoch   87/30 Cost: 2.331856\n",
      "Epoch   88/30 Cost: 1.779963\n",
      "Epoch   89/30 Cost: 2.223015\n",
      "Epoch   90/30 Cost: 2.929352\n",
      "Epoch   91/30 Cost: 3.036150\n",
      "Epoch   92/30 Cost: 2.466093\n",
      "Epoch   93/30 Cost: 1.871568\n",
      "Epoch   94/30 Cost: 1.826299\n",
      "Epoch   95/30 Cost: 2.226201\n",
      "Epoch   96/30 Cost: 2.509517\n",
      "Epoch   97/30 Cost: 2.342350\n",
      "Epoch   98/30 Cost: 1.945699\n",
      "Epoch   99/30 Cost: 1.756286\n",
      "Epoch  100/30 Cost: 1.908022\n",
      "Epoch  101/30 Cost: 2.139498\n",
      "Epoch  102/30 Cost: 2.151833\n",
      "Epoch  103/30 Cost: 1.943934\n",
      "Epoch  104/30 Cost: 1.761786\n",
      "Epoch  105/30 Cost: 1.782025\n",
      "Epoch  106/30 Cost: 1.924789\n",
      "Epoch  107/30 Cost: 1.986763\n",
      "Epoch  108/30 Cost: 1.894133\n",
      "Epoch  109/30 Cost: 1.764658\n",
      "Epoch  110/30 Cost: 1.738602\n",
      "Epoch  111/30 Cost: 1.813146\n",
      "Epoch  112/30 Cost: 1.871002\n",
      "Epoch  113/30 Cost: 1.836350\n",
      "Epoch  114/30 Cost: 1.755177\n",
      "Epoch  115/30 Cost: 1.720535\n",
      "Epoch  116/30 Cost: 1.755139\n",
      "Epoch  117/30 Cost: 1.796597\n",
      "Epoch  118/30 Cost: 1.785947\n",
      "Epoch  119/30 Cost: 1.737485\n",
      "Epoch  120/30 Cost: 1.708604\n",
      "Epoch  121/30 Cost: 1.723364\n",
      "Epoch  122/30 Cost: 1.749369\n",
      "Epoch  123/30 Cost: 1.746858\n",
      "Epoch  124/30 Cost: 1.718089\n",
      "Epoch  125/30 Cost: 1.697118\n",
      "Epoch  126/30 Cost: 1.702688\n",
      "Epoch  127/30 Cost: 1.717894\n",
      "Epoch  128/30 Cost: 1.717146\n",
      "Epoch  129/30 Cost: 1.699415\n",
      "Epoch  130/30 Cost: 1.685104\n",
      "Epoch  131/30 Cost: 1.686859\n",
      "Epoch  132/30 Cost: 1.695083\n",
      "Epoch  133/30 Cost: 1.694000\n",
      "Epoch  134/30 Cost: 1.682439\n",
      "Epoch  135/30 Cost: 1.672792\n",
      "Epoch  136/30 Cost: 1.672907\n",
      "Epoch  137/30 Cost: 1.676850\n",
      "Epoch  138/30 Cost: 1.674994\n",
      "Epoch  139/30 Cost: 1.666882\n",
      "Epoch  140/30 Cost: 1.660314\n",
      "Epoch  141/30 Cost: 1.659709\n",
      "Epoch  142/30 Cost: 1.661041\n",
      "Epoch  143/30 Cost: 1.658515\n",
      "Epoch  144/30 Cost: 1.652479\n",
      "Epoch  145/30 Cost: 1.647814\n",
      "Epoch  146/30 Cost: 1.646754\n",
      "Epoch  147/30 Cost: 1.646447\n",
      "Epoch  148/30 Cost: 1.643527\n",
      "Epoch  149/30 Cost: 1.638777\n",
      "Epoch  150/30 Cost: 1.635276\n",
      "Epoch  151/30 Cost: 1.633864\n",
      "Epoch  152/30 Cost: 1.632512\n",
      "Epoch  153/30 Cost: 1.629420\n",
      "Epoch  154/30 Cost: 1.625519\n",
      "Epoch  155/30 Cost: 1.622634\n",
      "Epoch  156/30 Cost: 1.620924\n",
      "Epoch  157/30 Cost: 1.618916\n",
      "Epoch  158/30 Cost: 1.615804\n",
      "Epoch  159/30 Cost: 1.612488\n",
      "Epoch  160/30 Cost: 1.609933\n",
      "Epoch  161/30 Cost: 1.607914\n",
      "Epoch  162/30 Cost: 1.605539\n",
      "Epoch  163/30 Cost: 1.602489\n",
      "Epoch  164/30 Cost: 1.599520\n",
      "Epoch  165/30 Cost: 1.597073\n",
      "Epoch  166/30 Cost: 1.594839\n",
      "Epoch  167/30 Cost: 1.592225\n",
      "Epoch  168/30 Cost: 1.589295\n",
      "Epoch  169/30 Cost: 1.586533\n",
      "Epoch  170/30 Cost: 1.584116\n",
      "Epoch  171/30 Cost: 1.581657\n",
      "Epoch  172/30 Cost: 1.578958\n",
      "Epoch  173/30 Cost: 1.576144\n",
      "Epoch  174/30 Cost: 1.573519\n",
      "Epoch  175/30 Cost: 1.571037\n",
      "Epoch  176/30 Cost: 1.568447\n",
      "Epoch  177/30 Cost: 1.565732\n",
      "Epoch  178/30 Cost: 1.562988\n",
      "Epoch  179/30 Cost: 1.560411\n",
      "Epoch  180/30 Cost: 1.557842\n",
      "Epoch  181/30 Cost: 1.555200\n",
      "Epoch  182/30 Cost: 1.552471\n",
      "Epoch  183/30 Cost: 1.549811\n",
      "Epoch  184/30 Cost: 1.547206\n",
      "Epoch  185/30 Cost: 1.544574\n",
      "Epoch  186/30 Cost: 1.541874\n",
      "Epoch  187/30 Cost: 1.539174\n",
      "Epoch  188/30 Cost: 1.536538\n",
      "Epoch  189/30 Cost: 1.533899\n",
      "Epoch  190/30 Cost: 1.531224\n",
      "Epoch  191/30 Cost: 1.528551\n",
      "Epoch  192/30 Cost: 1.525876\n",
      "Epoch  193/30 Cost: 1.523196\n",
      "Epoch  194/30 Cost: 1.520556\n",
      "Epoch  195/30 Cost: 1.517869\n",
      "Epoch  196/30 Cost: 1.515167\n",
      "Epoch  197/30 Cost: 1.512484\n",
      "Epoch  198/30 Cost: 1.509797\n",
      "Epoch  199/30 Cost: 1.507131\n",
      "Epoch  200/30 Cost: 1.504429\n",
      "Epoch  201/30 Cost: 1.501730\n",
      "Epoch  202/30 Cost: 1.499026\n",
      "Epoch  203/30 Cost: 1.496324\n",
      "Epoch  204/30 Cost: 1.493627\n",
      "Epoch  205/30 Cost: 1.490937\n",
      "Epoch  206/30 Cost: 1.488226\n",
      "Epoch  207/30 Cost: 1.485537\n",
      "Epoch  208/30 Cost: 1.482831\n",
      "Epoch  209/30 Cost: 1.480134\n",
      "Epoch  210/30 Cost: 1.477421\n",
      "Epoch  211/30 Cost: 1.474709\n",
      "Epoch  212/30 Cost: 1.471999\n",
      "Epoch  213/30 Cost: 1.469276\n",
      "Epoch  214/30 Cost: 1.466584\n",
      "Epoch  215/30 Cost: 1.463863\n",
      "Epoch  216/30 Cost: 1.461146\n",
      "Epoch  217/30 Cost: 1.458417\n",
      "Epoch  218/30 Cost: 1.455683\n",
      "Epoch  219/30 Cost: 1.452989\n",
      "Epoch  220/30 Cost: 1.450267\n",
      "Epoch  221/30 Cost: 1.447532\n",
      "Epoch  222/30 Cost: 1.444812\n",
      "Epoch  223/30 Cost: 1.442082\n",
      "Epoch  224/30 Cost: 1.439354\n",
      "Epoch  225/30 Cost: 1.436629\n",
      "Epoch  226/30 Cost: 1.433892\n",
      "Epoch  227/30 Cost: 1.431193\n",
      "Epoch  228/30 Cost: 1.428469\n",
      "Epoch  229/30 Cost: 1.425705\n",
      "Epoch  230/30 Cost: 1.422981\n",
      "Epoch  231/30 Cost: 1.420235\n",
      "Epoch  232/30 Cost: 1.417498\n",
      "Epoch  233/30 Cost: 1.414797\n",
      "Epoch  234/30 Cost: 1.412056\n",
      "Epoch  235/30 Cost: 1.409315\n",
      "Epoch  236/30 Cost: 1.406580\n",
      "Epoch  237/30 Cost: 1.403850\n",
      "Epoch  238/30 Cost: 1.401117\n",
      "Epoch  239/30 Cost: 1.398376\n",
      "Epoch  240/30 Cost: 1.395623\n",
      "Epoch  241/30 Cost: 1.392879\n",
      "Epoch  242/30 Cost: 1.390154\n",
      "Epoch  243/30 Cost: 1.387400\n",
      "Epoch  244/30 Cost: 1.384660\n",
      "Epoch  245/30 Cost: 1.381934\n",
      "Epoch  246/30 Cost: 1.379199\n",
      "Epoch  247/30 Cost: 1.376449\n",
      "Epoch  248/30 Cost: 1.373689\n",
      "Epoch  249/30 Cost: 1.370969\n",
      "Epoch  250/30 Cost: 1.368218\n",
      "Epoch  251/30 Cost: 1.365480\n",
      "Epoch  252/30 Cost: 1.362734\n",
      "Epoch  253/30 Cost: 1.359988\n",
      "Epoch  254/30 Cost: 1.357268\n",
      "Epoch  255/30 Cost: 1.354509\n",
      "Epoch  256/30 Cost: 1.351764\n",
      "Epoch  257/30 Cost: 1.349025\n",
      "Epoch  258/30 Cost: 1.346310\n",
      "Epoch  259/30 Cost: 1.343561\n",
      "Epoch  260/30 Cost: 1.340815\n",
      "Epoch  261/30 Cost: 1.338084\n",
      "Epoch  262/30 Cost: 1.335355\n",
      "Epoch  263/30 Cost: 1.332594\n",
      "Epoch  264/30 Cost: 1.329875\n",
      "Epoch  265/30 Cost: 1.327114\n",
      "Epoch  266/30 Cost: 1.324379\n",
      "Epoch  267/30 Cost: 1.321646\n",
      "Epoch  268/30 Cost: 1.318893\n",
      "Epoch  269/30 Cost: 1.316171\n",
      "Epoch  270/30 Cost: 1.313428\n",
      "Epoch  271/30 Cost: 1.310713\n",
      "Epoch  272/30 Cost: 1.307967\n",
      "Epoch  273/30 Cost: 1.305236\n",
      "Epoch  274/30 Cost: 1.302488\n",
      "Epoch  275/30 Cost: 1.299756\n",
      "Epoch  276/30 Cost: 1.297033\n",
      "Epoch  277/30 Cost: 1.294307\n",
      "Epoch  278/30 Cost: 1.291583\n",
      "Epoch  279/30 Cost: 1.288860\n",
      "Epoch  280/30 Cost: 1.286120\n",
      "Epoch  281/30 Cost: 1.283394\n",
      "Epoch  282/30 Cost: 1.280671\n",
      "Epoch  283/30 Cost: 1.277949\n",
      "Epoch  284/30 Cost: 1.275210\n",
      "Epoch  285/30 Cost: 1.272476\n",
      "Epoch  286/30 Cost: 1.269758\n",
      "Epoch  287/30 Cost: 1.267032\n",
      "Epoch  288/30 Cost: 1.264311\n",
      "Epoch  289/30 Cost: 1.261602\n",
      "Epoch  290/30 Cost: 1.258877\n",
      "Epoch  291/30 Cost: 1.256167\n",
      "Epoch  292/30 Cost: 1.253444\n",
      "Epoch  293/30 Cost: 1.250726\n",
      "Epoch  294/30 Cost: 1.248033\n",
      "Epoch  295/30 Cost: 1.245319\n",
      "Epoch  296/30 Cost: 1.242609\n",
      "Epoch  297/30 Cost: 1.239887\n",
      "Epoch  298/30 Cost: 1.237175\n",
      "Epoch  299/30 Cost: 1.234484\n",
      "Epoch  300/30 Cost: 1.231776\n",
      "Epoch  301/30 Cost: 1.229069\n",
      "Epoch  302/30 Cost: 1.226371\n",
      "Epoch  303/30 Cost: 1.223660\n",
      "Epoch  304/30 Cost: 1.220963\n",
      "Epoch  305/30 Cost: 1.218270\n",
      "Epoch  306/30 Cost: 1.215569\n",
      "Epoch  307/30 Cost: 1.212883\n",
      "Epoch  308/30 Cost: 1.210196\n",
      "Epoch  309/30 Cost: 1.207496\n",
      "Epoch  310/30 Cost: 1.204810\n",
      "Epoch  311/30 Cost: 1.202132\n",
      "Epoch  312/30 Cost: 1.199424\n",
      "Epoch  313/30 Cost: 1.196749\n",
      "Epoch  314/30 Cost: 1.194081\n",
      "Epoch  315/30 Cost: 1.191393\n",
      "Epoch  316/30 Cost: 1.188716\n",
      "Epoch  317/30 Cost: 1.186034\n",
      "Epoch  318/30 Cost: 1.183364\n",
      "Epoch  319/30 Cost: 1.180697\n",
      "Epoch  320/30 Cost: 1.178018\n",
      "Epoch  321/30 Cost: 1.175339\n",
      "Epoch  322/30 Cost: 1.172690\n",
      "Epoch  323/30 Cost: 1.170010\n",
      "Epoch  324/30 Cost: 1.167358\n",
      "Epoch  325/30 Cost: 1.164686\n",
      "Epoch  326/30 Cost: 1.162027\n",
      "Epoch  327/30 Cost: 1.159391\n",
      "Epoch  328/30 Cost: 1.156713\n",
      "Epoch  329/30 Cost: 1.154079\n",
      "Epoch  330/30 Cost: 1.151421\n",
      "Epoch  331/30 Cost: 1.148785\n",
      "Epoch  332/30 Cost: 1.146122\n",
      "Epoch  333/30 Cost: 1.143482\n",
      "Epoch  334/30 Cost: 1.140820\n",
      "Epoch  335/30 Cost: 1.138189\n",
      "Epoch  336/30 Cost: 1.135568\n",
      "Epoch  337/30 Cost: 1.132927\n",
      "Epoch  338/30 Cost: 1.130300\n",
      "Epoch  339/30 Cost: 1.127669\n",
      "Epoch  340/30 Cost: 1.125041\n",
      "Epoch  341/30 Cost: 1.122427\n",
      "Epoch  342/30 Cost: 1.119789\n",
      "Epoch  343/30 Cost: 1.117155\n",
      "Epoch  344/30 Cost: 1.114571\n",
      "Epoch  345/30 Cost: 1.111945\n",
      "Epoch  346/30 Cost: 1.109329\n",
      "Epoch  347/30 Cost: 1.106726\n",
      "Epoch  348/30 Cost: 1.104126\n",
      "Epoch  349/30 Cost: 1.101501\n",
      "Epoch  350/30 Cost: 1.098907\n",
      "Epoch  351/30 Cost: 1.096314\n",
      "Epoch  352/30 Cost: 1.093704\n",
      "Epoch  353/30 Cost: 1.091111\n",
      "Epoch  354/30 Cost: 1.088521\n",
      "Epoch  355/30 Cost: 1.085915\n",
      "Epoch  356/30 Cost: 1.083361\n",
      "Epoch  357/30 Cost: 1.080747\n",
      "Epoch  358/30 Cost: 1.078182\n",
      "Epoch  359/30 Cost: 1.075603\n",
      "Epoch  360/30 Cost: 1.073019\n",
      "Epoch  361/30 Cost: 1.070450\n",
      "Epoch  362/30 Cost: 1.067885\n",
      "Epoch  363/30 Cost: 1.065308\n",
      "Epoch  364/30 Cost: 1.062745\n",
      "Epoch  365/30 Cost: 1.060186\n",
      "Epoch  366/30 Cost: 1.057639\n",
      "Epoch  367/30 Cost: 1.055078\n",
      "Epoch  368/30 Cost: 1.052527\n",
      "Epoch  369/30 Cost: 1.049979\n",
      "Epoch  370/30 Cost: 1.047421\n",
      "Epoch  371/30 Cost: 1.044883\n",
      "Epoch  372/30 Cost: 1.042340\n",
      "Epoch  373/30 Cost: 1.039803\n",
      "Epoch  374/30 Cost: 1.037264\n",
      "Epoch  375/30 Cost: 1.034731\n",
      "Epoch  376/30 Cost: 1.032218\n",
      "Epoch  377/30 Cost: 1.029709\n",
      "Epoch  378/30 Cost: 1.027172\n",
      "Epoch  379/30 Cost: 1.024648\n",
      "Epoch  380/30 Cost: 1.022130\n",
      "Epoch  381/30 Cost: 1.019628\n",
      "Epoch  382/30 Cost: 1.017105\n",
      "Epoch  383/30 Cost: 1.014600\n",
      "Epoch  384/30 Cost: 1.012084\n",
      "Epoch  385/30 Cost: 1.009583\n",
      "Epoch  386/30 Cost: 1.007104\n",
      "Epoch  387/30 Cost: 1.004605\n",
      "Epoch  388/30 Cost: 1.002099\n",
      "Epoch  389/30 Cost: 0.999615\n",
      "Epoch  390/30 Cost: 0.997129\n",
      "Epoch  391/30 Cost: 0.994639\n",
      "Epoch  392/30 Cost: 0.992177\n",
      "Epoch  393/30 Cost: 0.989714\n",
      "Epoch  394/30 Cost: 0.987215\n",
      "Epoch  395/30 Cost: 0.984773\n",
      "Epoch  396/30 Cost: 0.982298\n",
      "Epoch  397/30 Cost: 0.979827\n",
      "Epoch  398/30 Cost: 0.977376\n",
      "Epoch  399/30 Cost: 0.974925\n",
      "Epoch  400/30 Cost: 0.972453\n",
      "Epoch  401/30 Cost: 0.970012\n",
      "Epoch  402/30 Cost: 0.967573\n",
      "Epoch  403/30 Cost: 0.965132\n",
      "Epoch  404/30 Cost: 0.962689\n",
      "Epoch  405/30 Cost: 0.960263\n",
      "Epoch  406/30 Cost: 0.957842\n",
      "Epoch  407/30 Cost: 0.955417\n",
      "Epoch  408/30 Cost: 0.952986\n",
      "Epoch  409/30 Cost: 0.950564\n",
      "Epoch  410/30 Cost: 0.948144\n",
      "Epoch  411/30 Cost: 0.945729\n",
      "Epoch  412/30 Cost: 0.943318\n",
      "Epoch  413/30 Cost: 0.940906\n",
      "Epoch  414/30 Cost: 0.938522\n",
      "Epoch  415/30 Cost: 0.936119\n",
      "Epoch  416/30 Cost: 0.933714\n",
      "Epoch  417/30 Cost: 0.931335\n",
      "Epoch  418/30 Cost: 0.928948\n",
      "Epoch  419/30 Cost: 0.926562\n",
      "Epoch  420/30 Cost: 0.924176\n",
      "Epoch  421/30 Cost: 0.921802\n",
      "Epoch  422/30 Cost: 0.919442\n",
      "Epoch  423/30 Cost: 0.917055\n",
      "Epoch  424/30 Cost: 0.914706\n",
      "Epoch  425/30 Cost: 0.912323\n",
      "Epoch  426/30 Cost: 0.909975\n",
      "Epoch  427/30 Cost: 0.907600\n",
      "Epoch  428/30 Cost: 0.905260\n",
      "Epoch  429/30 Cost: 0.902919\n",
      "Epoch  430/30 Cost: 0.900584\n",
      "Epoch  431/30 Cost: 0.898255\n",
      "Epoch  432/30 Cost: 0.895907\n",
      "Epoch  433/30 Cost: 0.893571\n",
      "Epoch  434/30 Cost: 0.891227\n",
      "Epoch  435/30 Cost: 0.888913\n",
      "Epoch  436/30 Cost: 0.886592\n",
      "Epoch  437/30 Cost: 0.884283\n",
      "Epoch  438/30 Cost: 0.881967\n",
      "Epoch  439/30 Cost: 0.879637\n",
      "Epoch  440/30 Cost: 0.877353\n",
      "Epoch  441/30 Cost: 0.875063\n",
      "Epoch  442/30 Cost: 0.872748\n",
      "Epoch  443/30 Cost: 0.870442\n",
      "Epoch  444/30 Cost: 0.868155\n",
      "Epoch  445/30 Cost: 0.865889\n",
      "Epoch  446/30 Cost: 0.863616\n",
      "Epoch  447/30 Cost: 0.861323\n",
      "Epoch  448/30 Cost: 0.859045\n",
      "Epoch  449/30 Cost: 0.856770\n",
      "Epoch  450/30 Cost: 0.854498\n",
      "Epoch  451/30 Cost: 0.852247\n",
      "Epoch  452/30 Cost: 0.849992\n",
      "Epoch  453/30 Cost: 0.847726\n",
      "Epoch  454/30 Cost: 0.845478\n",
      "Epoch  455/30 Cost: 0.843237\n",
      "Epoch  456/30 Cost: 0.840992\n",
      "Epoch  457/30 Cost: 0.838758\n",
      "Epoch  458/30 Cost: 0.836526\n",
      "Epoch  459/30 Cost: 0.834294\n",
      "Epoch  460/30 Cost: 0.832060\n",
      "Epoch  461/30 Cost: 0.829848\n",
      "Epoch  462/30 Cost: 0.827612\n",
      "Epoch  463/30 Cost: 0.825409\n",
      "Epoch  464/30 Cost: 0.823189\n",
      "Epoch  465/30 Cost: 0.820983\n",
      "Epoch  466/30 Cost: 0.818781\n",
      "Epoch  467/30 Cost: 0.816605\n",
      "Epoch  468/30 Cost: 0.814383\n",
      "Epoch  469/30 Cost: 0.812193\n",
      "Epoch  470/30 Cost: 0.810012\n",
      "Epoch  471/30 Cost: 0.807834\n",
      "Epoch  472/30 Cost: 0.805669\n",
      "Epoch  473/30 Cost: 0.803471\n",
      "Epoch  474/30 Cost: 0.801311\n",
      "Epoch  475/30 Cost: 0.799159\n",
      "Epoch  476/30 Cost: 0.796975\n",
      "Epoch  477/30 Cost: 0.794840\n",
      "Epoch  478/30 Cost: 0.792664\n",
      "Epoch  479/30 Cost: 0.790525\n",
      "Epoch  480/30 Cost: 0.788376\n",
      "Epoch  481/30 Cost: 0.786241\n",
      "Epoch  482/30 Cost: 0.784094\n",
      "Epoch  483/30 Cost: 0.781953\n",
      "Epoch  484/30 Cost: 0.779840\n",
      "Epoch  485/30 Cost: 0.777724\n",
      "Epoch  486/30 Cost: 0.775592\n",
      "Epoch  487/30 Cost: 0.773472\n",
      "Epoch  488/30 Cost: 0.771373\n",
      "Epoch  489/30 Cost: 0.769265\n",
      "Epoch  490/30 Cost: 0.767149\n",
      "Epoch  491/30 Cost: 0.765053\n",
      "Epoch  492/30 Cost: 0.762956\n",
      "Epoch  493/30 Cost: 0.760872\n",
      "Epoch  494/30 Cost: 0.758785\n",
      "Epoch  495/30 Cost: 0.756693\n",
      "Epoch  496/30 Cost: 0.754624\n",
      "Epoch  497/30 Cost: 0.752560\n",
      "Epoch  498/30 Cost: 0.750490\n",
      "Epoch  499/30 Cost: 0.748421\n",
      "Epoch  500/30 Cost: 0.746353\n",
      "Epoch  501/30 Cost: 0.744300\n",
      "Epoch  502/30 Cost: 0.742245\n",
      "Epoch  503/30 Cost: 0.740187\n",
      "Epoch  504/30 Cost: 0.738141\n",
      "Epoch  505/30 Cost: 0.736101\n",
      "Epoch  506/30 Cost: 0.734074\n",
      "Epoch  507/30 Cost: 0.732053\n",
      "Epoch  508/30 Cost: 0.730025\n",
      "Epoch  509/30 Cost: 0.727992\n",
      "Epoch  510/30 Cost: 0.725988\n",
      "Epoch  511/30 Cost: 0.723960\n",
      "Epoch  512/30 Cost: 0.721959\n",
      "Epoch  513/30 Cost: 0.719929\n",
      "Epoch  514/30 Cost: 0.717953\n",
      "Epoch  515/30 Cost: 0.715938\n",
      "Epoch  516/30 Cost: 0.713959\n",
      "Epoch  517/30 Cost: 0.711964\n",
      "Epoch  518/30 Cost: 0.709978\n",
      "Epoch  519/30 Cost: 0.708003\n",
      "Epoch  520/30 Cost: 0.706031\n",
      "Epoch  521/30 Cost: 0.704053\n",
      "Epoch  522/30 Cost: 0.702085\n",
      "Epoch  523/30 Cost: 0.700130\n",
      "Epoch  524/30 Cost: 0.698159\n",
      "Epoch  525/30 Cost: 0.696221\n",
      "Epoch  526/30 Cost: 0.694266\n",
      "Epoch  527/30 Cost: 0.692310\n",
      "Epoch  528/30 Cost: 0.690380\n",
      "Epoch  529/30 Cost: 0.688444\n",
      "Epoch  530/30 Cost: 0.686504\n",
      "Epoch  531/30 Cost: 0.684577\n",
      "Epoch  532/30 Cost: 0.682654\n",
      "Epoch  533/30 Cost: 0.680733\n",
      "Epoch  534/30 Cost: 0.678824\n",
      "Epoch  535/30 Cost: 0.676903\n",
      "Epoch  536/30 Cost: 0.675007\n",
      "Epoch  537/30 Cost: 0.673100\n",
      "Epoch  538/30 Cost: 0.671206\n",
      "Epoch  539/30 Cost: 0.669309\n",
      "Epoch  540/30 Cost: 0.667419\n",
      "Epoch  541/30 Cost: 0.665535\n",
      "Epoch  542/30 Cost: 0.663656\n",
      "Epoch  543/30 Cost: 0.661770\n",
      "Epoch  544/30 Cost: 0.659917\n",
      "Epoch  545/30 Cost: 0.658030\n",
      "Epoch  546/30 Cost: 0.656168\n",
      "Epoch  547/30 Cost: 0.654321\n",
      "Epoch  548/30 Cost: 0.652475\n",
      "Epoch  549/30 Cost: 0.650612\n",
      "Epoch  550/30 Cost: 0.648771\n",
      "Epoch  551/30 Cost: 0.646922\n",
      "Epoch  552/30 Cost: 0.645099\n",
      "Epoch  553/30 Cost: 0.643258\n",
      "Epoch  554/30 Cost: 0.641427\n",
      "Epoch  555/30 Cost: 0.639600\n",
      "Epoch  556/30 Cost: 0.637795\n",
      "Epoch  557/30 Cost: 0.635969\n",
      "Epoch  558/30 Cost: 0.634153\n",
      "Epoch  559/30 Cost: 0.632340\n",
      "Epoch  560/30 Cost: 0.630549\n",
      "Epoch  561/30 Cost: 0.628759\n",
      "Epoch  562/30 Cost: 0.626957\n",
      "Epoch  563/30 Cost: 0.625169\n",
      "Epoch  564/30 Cost: 0.623387\n",
      "Epoch  565/30 Cost: 0.621607\n",
      "Epoch  566/30 Cost: 0.619831\n",
      "Epoch  567/30 Cost: 0.618053\n",
      "Epoch  568/30 Cost: 0.616302\n",
      "Epoch  569/30 Cost: 0.614520\n",
      "Epoch  570/30 Cost: 0.612766\n",
      "Epoch  571/30 Cost: 0.611017\n",
      "Epoch  572/30 Cost: 0.609262\n",
      "Epoch  573/30 Cost: 0.607514\n",
      "Epoch  574/30 Cost: 0.605769\n",
      "Epoch  575/30 Cost: 0.604042\n",
      "Epoch  576/30 Cost: 0.602307\n",
      "Epoch  577/30 Cost: 0.600582\n",
      "Epoch  578/30 Cost: 0.598843\n",
      "Epoch  579/30 Cost: 0.597144\n",
      "Epoch  580/30 Cost: 0.595423\n",
      "Epoch  581/30 Cost: 0.593721\n",
      "Epoch  582/30 Cost: 0.592003\n",
      "Epoch  583/30 Cost: 0.590304\n",
      "Epoch  584/30 Cost: 0.588599\n",
      "Epoch  585/30 Cost: 0.586909\n",
      "Epoch  586/30 Cost: 0.585218\n",
      "Epoch  587/30 Cost: 0.583543\n",
      "Epoch  588/30 Cost: 0.581863\n",
      "Epoch  589/30 Cost: 0.580192\n",
      "Epoch  590/30 Cost: 0.578503\n",
      "Epoch  591/30 Cost: 0.576837\n",
      "Epoch  592/30 Cost: 0.575176\n",
      "Epoch  593/30 Cost: 0.573522\n",
      "Epoch  594/30 Cost: 0.571871\n",
      "Epoch  595/30 Cost: 0.570213\n",
      "Epoch  596/30 Cost: 0.568569\n",
      "Epoch  597/30 Cost: 0.566932\n",
      "Epoch  598/30 Cost: 0.565277\n",
      "Epoch  599/30 Cost: 0.563663\n",
      "Epoch  600/30 Cost: 0.562027\n",
      "Epoch  601/30 Cost: 0.560402\n",
      "Epoch  602/30 Cost: 0.558785\n",
      "Epoch  603/30 Cost: 0.557167\n",
      "Epoch  604/30 Cost: 0.555570\n",
      "Epoch  605/30 Cost: 0.553945\n",
      "Epoch  606/30 Cost: 0.552338\n",
      "Epoch  607/30 Cost: 0.550745\n",
      "Epoch  608/30 Cost: 0.549161\n",
      "Epoch  609/30 Cost: 0.547564\n",
      "Epoch  610/30 Cost: 0.545972\n",
      "Epoch  611/30 Cost: 0.544405\n",
      "Epoch  612/30 Cost: 0.542818\n",
      "Epoch  613/30 Cost: 0.541243\n",
      "Epoch  614/30 Cost: 0.539672\n",
      "Epoch  615/30 Cost: 0.538112\n",
      "Epoch  616/30 Cost: 0.536564\n",
      "Epoch  617/30 Cost: 0.534992\n",
      "Epoch  618/30 Cost: 0.533438\n",
      "Epoch  619/30 Cost: 0.531903\n",
      "Epoch  620/30 Cost: 0.530355\n",
      "Epoch  621/30 Cost: 0.528810\n",
      "Epoch  622/30 Cost: 0.527280\n",
      "Epoch  623/30 Cost: 0.525755\n",
      "Epoch  624/30 Cost: 0.524232\n",
      "Epoch  625/30 Cost: 0.522707\n",
      "Epoch  626/30 Cost: 0.521188\n",
      "Epoch  627/30 Cost: 0.519675\n",
      "Epoch  628/30 Cost: 0.518155\n",
      "Epoch  629/30 Cost: 0.516663\n",
      "Epoch  630/30 Cost: 0.515159\n",
      "Epoch  631/30 Cost: 0.513673\n",
      "Epoch  632/30 Cost: 0.512169\n",
      "Epoch  633/30 Cost: 0.510672\n",
      "Epoch  634/30 Cost: 0.509200\n",
      "Epoch  635/30 Cost: 0.507719\n",
      "Epoch  636/30 Cost: 0.506240\n",
      "Epoch  637/30 Cost: 0.504769\n",
      "Epoch  638/30 Cost: 0.503306\n",
      "Epoch  639/30 Cost: 0.501837\n",
      "Epoch  640/30 Cost: 0.500385\n",
      "Epoch  641/30 Cost: 0.498933\n",
      "Epoch  642/30 Cost: 0.497480\n",
      "Epoch  643/30 Cost: 0.496032\n",
      "Epoch  644/30 Cost: 0.494595\n",
      "Epoch  645/30 Cost: 0.493154\n",
      "Epoch  646/30 Cost: 0.491721\n",
      "Epoch  647/30 Cost: 0.490293\n",
      "Epoch  648/30 Cost: 0.488871\n",
      "Epoch  649/30 Cost: 0.487437\n",
      "Epoch  650/30 Cost: 0.486032\n",
      "Epoch  651/30 Cost: 0.484613\n",
      "Epoch  652/30 Cost: 0.483203\n",
      "Epoch  653/30 Cost: 0.481786\n",
      "Epoch  654/30 Cost: 0.480395\n",
      "Epoch  655/30 Cost: 0.479000\n",
      "Epoch  656/30 Cost: 0.477614\n",
      "Epoch  657/30 Cost: 0.476219\n",
      "Epoch  658/30 Cost: 0.474836\n",
      "Epoch  659/30 Cost: 0.473453\n",
      "Epoch  660/30 Cost: 0.472073\n",
      "Epoch  661/30 Cost: 0.470718\n",
      "Epoch  662/30 Cost: 0.469346\n",
      "Epoch  663/30 Cost: 0.467966\n",
      "Epoch  664/30 Cost: 0.466622\n",
      "Epoch  665/30 Cost: 0.465256\n",
      "Epoch  666/30 Cost: 0.463921\n",
      "Epoch  667/30 Cost: 0.462572\n",
      "Epoch  668/30 Cost: 0.461220\n",
      "Epoch  669/30 Cost: 0.459874\n",
      "Epoch  670/30 Cost: 0.458547\n",
      "Epoch  671/30 Cost: 0.457213\n",
      "Epoch  672/30 Cost: 0.455892\n",
      "Epoch  673/30 Cost: 0.454569\n",
      "Epoch  674/30 Cost: 0.453242\n",
      "Epoch  675/30 Cost: 0.451938\n",
      "Epoch  676/30 Cost: 0.450624\n",
      "Epoch  677/30 Cost: 0.449314\n",
      "Epoch  678/30 Cost: 0.448007\n",
      "Epoch  679/30 Cost: 0.446717\n",
      "Epoch  680/30 Cost: 0.445412\n",
      "Epoch  681/30 Cost: 0.444121\n",
      "Epoch  682/30 Cost: 0.442848\n",
      "Epoch  683/30 Cost: 0.441560\n",
      "Epoch  684/30 Cost: 0.440281\n",
      "Epoch  685/30 Cost: 0.438992\n",
      "Epoch  686/30 Cost: 0.437729\n",
      "Epoch  687/30 Cost: 0.436454\n",
      "Epoch  688/30 Cost: 0.435203\n",
      "Epoch  689/30 Cost: 0.433936\n",
      "Epoch  690/30 Cost: 0.432686\n",
      "Epoch  691/30 Cost: 0.431433\n",
      "Epoch  692/30 Cost: 0.430184\n",
      "Epoch  693/30 Cost: 0.428947\n",
      "Epoch  694/30 Cost: 0.427707\n",
      "Epoch  695/30 Cost: 0.426467\n",
      "Epoch  696/30 Cost: 0.425231\n",
      "Epoch  697/30 Cost: 0.424016\n",
      "Epoch  698/30 Cost: 0.422786\n",
      "Epoch  699/30 Cost: 0.421556\n",
      "Epoch  700/30 Cost: 0.420351\n",
      "Epoch  701/30 Cost: 0.419125\n",
      "Epoch  702/30 Cost: 0.417928\n",
      "Epoch  703/30 Cost: 0.416717\n",
      "Epoch  704/30 Cost: 0.415524\n",
      "Epoch  705/30 Cost: 0.414318\n",
      "Epoch  706/30 Cost: 0.413135\n",
      "Epoch  707/30 Cost: 0.411943\n",
      "Epoch  708/30 Cost: 0.410748\n",
      "Epoch  709/30 Cost: 0.409582\n",
      "Epoch  710/30 Cost: 0.408388\n",
      "Epoch  711/30 Cost: 0.407223\n",
      "Epoch  712/30 Cost: 0.406046\n",
      "Epoch  713/30 Cost: 0.404893\n",
      "Epoch  714/30 Cost: 0.403721\n",
      "Epoch  715/30 Cost: 0.402567\n",
      "Epoch  716/30 Cost: 0.401408\n",
      "Epoch  717/30 Cost: 0.400261\n",
      "Epoch  718/30 Cost: 0.399117\n",
      "Epoch  719/30 Cost: 0.397969\n",
      "Epoch  720/30 Cost: 0.396835\n",
      "Epoch  721/30 Cost: 0.395679\n",
      "Epoch  722/30 Cost: 0.394552\n",
      "Epoch  723/30 Cost: 0.393422\n",
      "Epoch  724/30 Cost: 0.392313\n",
      "Epoch  725/30 Cost: 0.391186\n",
      "Epoch  726/30 Cost: 0.390056\n",
      "Epoch  727/30 Cost: 0.388951\n",
      "Epoch  728/30 Cost: 0.387844\n",
      "Epoch  729/30 Cost: 0.386737\n",
      "Epoch  730/30 Cost: 0.385641\n",
      "Epoch  731/30 Cost: 0.384531\n",
      "Epoch  732/30 Cost: 0.383442\n",
      "Epoch  733/30 Cost: 0.382360\n",
      "Epoch  734/30 Cost: 0.381255\n",
      "Epoch  735/30 Cost: 0.380176\n",
      "Epoch  736/30 Cost: 0.379101\n",
      "Epoch  737/30 Cost: 0.378013\n",
      "Epoch  738/30 Cost: 0.376944\n",
      "Epoch  739/30 Cost: 0.375877\n",
      "Epoch  740/30 Cost: 0.374814\n",
      "Epoch  741/30 Cost: 0.373746\n",
      "Epoch  742/30 Cost: 0.372690\n",
      "Epoch  743/30 Cost: 0.371643\n",
      "Epoch  744/30 Cost: 0.370579\n",
      "Epoch  745/30 Cost: 0.369534\n",
      "Epoch  746/30 Cost: 0.368494\n",
      "Epoch  747/30 Cost: 0.367457\n",
      "Epoch  748/30 Cost: 0.366412\n",
      "Epoch  749/30 Cost: 0.365377\n",
      "Epoch  750/30 Cost: 0.364344\n",
      "Epoch  751/30 Cost: 0.363335\n",
      "Epoch  752/30 Cost: 0.362293\n",
      "Epoch  753/30 Cost: 0.361286\n",
      "Epoch  754/30 Cost: 0.360261\n",
      "Epoch  755/30 Cost: 0.359249\n",
      "Epoch  756/30 Cost: 0.358255\n",
      "Epoch  757/30 Cost: 0.357240\n",
      "Epoch  758/30 Cost: 0.356223\n",
      "Epoch  759/30 Cost: 0.355243\n",
      "Epoch  760/30 Cost: 0.354244\n",
      "Epoch  761/30 Cost: 0.353253\n",
      "Epoch  762/30 Cost: 0.352269\n",
      "Epoch  763/30 Cost: 0.351290\n",
      "Epoch  764/30 Cost: 0.350305\n",
      "Epoch  765/30 Cost: 0.349314\n",
      "Epoch  766/30 Cost: 0.348348\n",
      "Epoch  767/30 Cost: 0.347373\n",
      "Epoch  768/30 Cost: 0.346421\n",
      "Epoch  769/30 Cost: 0.345448\n",
      "Epoch  770/30 Cost: 0.344486\n",
      "Epoch  771/30 Cost: 0.343536\n",
      "Epoch  772/30 Cost: 0.342581\n",
      "Epoch  773/30 Cost: 0.341632\n",
      "Epoch  774/30 Cost: 0.340678\n",
      "Epoch  775/30 Cost: 0.339747\n",
      "Epoch  776/30 Cost: 0.338800\n",
      "Epoch  777/30 Cost: 0.337867\n",
      "Epoch  778/30 Cost: 0.336933\n",
      "Epoch  779/30 Cost: 0.336006\n",
      "Epoch  780/30 Cost: 0.335072\n",
      "Epoch  781/30 Cost: 0.334157\n",
      "Epoch  782/30 Cost: 0.333234\n",
      "Epoch  783/30 Cost: 0.332317\n",
      "Epoch  784/30 Cost: 0.331404\n",
      "Epoch  785/30 Cost: 0.330491\n",
      "Epoch  786/30 Cost: 0.329583\n",
      "Epoch  787/30 Cost: 0.328677\n",
      "Epoch  788/30 Cost: 0.327785\n",
      "Epoch  789/30 Cost: 0.326889\n",
      "Epoch  790/30 Cost: 0.325994\n",
      "Epoch  791/30 Cost: 0.325098\n",
      "Epoch  792/30 Cost: 0.324217\n",
      "Epoch  793/30 Cost: 0.323332\n",
      "Epoch  794/30 Cost: 0.322450\n",
      "Epoch  795/30 Cost: 0.321573\n",
      "Epoch  796/30 Cost: 0.320702\n",
      "Epoch  797/30 Cost: 0.319828\n",
      "Epoch  798/30 Cost: 0.318956\n",
      "Epoch  799/30 Cost: 0.318100\n",
      "Epoch  800/30 Cost: 0.317245\n",
      "Epoch  801/30 Cost: 0.316383\n",
      "Epoch  802/30 Cost: 0.315519\n",
      "Epoch  803/30 Cost: 0.314673\n",
      "Epoch  804/30 Cost: 0.313823\n",
      "Epoch  805/30 Cost: 0.312973\n",
      "Epoch  806/30 Cost: 0.312144\n",
      "Epoch  807/30 Cost: 0.311302\n",
      "Epoch  808/30 Cost: 0.310460\n",
      "Epoch  809/30 Cost: 0.309631\n",
      "Epoch  810/30 Cost: 0.308797\n",
      "Epoch  811/30 Cost: 0.307982\n",
      "Epoch  812/30 Cost: 0.307158\n",
      "Epoch  813/30 Cost: 0.306333\n",
      "Epoch  814/30 Cost: 0.305519\n",
      "Epoch  815/30 Cost: 0.304702\n",
      "Epoch  816/30 Cost: 0.303906\n",
      "Epoch  817/30 Cost: 0.303087\n",
      "Epoch  818/30 Cost: 0.302285\n",
      "Epoch  819/30 Cost: 0.301484\n",
      "Epoch  820/30 Cost: 0.300684\n",
      "Epoch  821/30 Cost: 0.299885\n",
      "Epoch  822/30 Cost: 0.299098\n",
      "Epoch  823/30 Cost: 0.298304\n",
      "Epoch  824/30 Cost: 0.297524\n",
      "Epoch  825/30 Cost: 0.296737\n",
      "Epoch  826/30 Cost: 0.295949\n",
      "Epoch  827/30 Cost: 0.295179\n",
      "Epoch  828/30 Cost: 0.294417\n",
      "Epoch  829/30 Cost: 0.293628\n",
      "Epoch  830/30 Cost: 0.292866\n",
      "Epoch  831/30 Cost: 0.292105\n",
      "Epoch  832/30 Cost: 0.291338\n",
      "Epoch  833/30 Cost: 0.290578\n",
      "Epoch  834/30 Cost: 0.289824\n",
      "Epoch  835/30 Cost: 0.289073\n",
      "Epoch  836/30 Cost: 0.288311\n",
      "Epoch  837/30 Cost: 0.287565\n",
      "Epoch  838/30 Cost: 0.286828\n",
      "Epoch  839/30 Cost: 0.286090\n",
      "Epoch  840/30 Cost: 0.285342\n",
      "Epoch  841/30 Cost: 0.284609\n",
      "Epoch  842/30 Cost: 0.283880\n",
      "Epoch  843/30 Cost: 0.283157\n",
      "Epoch  844/30 Cost: 0.282413\n",
      "Epoch  845/30 Cost: 0.281698\n",
      "Epoch  846/30 Cost: 0.280970\n",
      "Epoch  847/30 Cost: 0.280255\n",
      "Epoch  848/30 Cost: 0.279538\n",
      "Epoch  849/30 Cost: 0.278817\n",
      "Epoch  850/30 Cost: 0.278122\n",
      "Epoch  851/30 Cost: 0.277405\n",
      "Epoch  852/30 Cost: 0.276703\n",
      "Epoch  853/30 Cost: 0.276009\n",
      "Epoch  854/30 Cost: 0.275305\n",
      "Epoch  855/30 Cost: 0.274613\n",
      "Epoch  856/30 Cost: 0.273909\n",
      "Epoch  857/30 Cost: 0.273227\n",
      "Epoch  858/30 Cost: 0.272537\n",
      "Epoch  859/30 Cost: 0.271858\n",
      "Epoch  860/30 Cost: 0.271167\n",
      "Epoch  861/30 Cost: 0.270485\n",
      "Epoch  862/30 Cost: 0.269808\n",
      "Epoch  863/30 Cost: 0.269140\n",
      "Epoch  864/30 Cost: 0.268464\n",
      "Epoch  865/30 Cost: 0.267788\n",
      "Epoch  866/30 Cost: 0.267126\n",
      "Epoch  867/30 Cost: 0.266462\n",
      "Epoch  868/30 Cost: 0.265806\n",
      "Epoch  869/30 Cost: 0.265147\n",
      "Epoch  870/30 Cost: 0.264493\n",
      "Epoch  871/30 Cost: 0.263839\n",
      "Epoch  872/30 Cost: 0.263183\n",
      "Epoch  873/30 Cost: 0.262542\n",
      "Epoch  874/30 Cost: 0.261902\n",
      "Epoch  875/30 Cost: 0.261267\n",
      "Epoch  876/30 Cost: 0.260618\n",
      "Epoch  877/30 Cost: 0.259976\n",
      "Epoch  878/30 Cost: 0.259351\n",
      "Epoch  879/30 Cost: 0.258712\n",
      "Epoch  880/30 Cost: 0.258077\n",
      "Epoch  881/30 Cost: 0.257465\n",
      "Epoch  882/30 Cost: 0.256847\n",
      "Epoch  883/30 Cost: 0.256213\n",
      "Epoch  884/30 Cost: 0.255606\n",
      "Epoch  885/30 Cost: 0.254985\n",
      "Epoch  886/30 Cost: 0.254363\n",
      "Epoch  887/30 Cost: 0.253762\n",
      "Epoch  888/30 Cost: 0.253146\n",
      "Epoch  889/30 Cost: 0.252538\n",
      "Epoch  890/30 Cost: 0.251937\n",
      "Epoch  891/30 Cost: 0.251326\n",
      "Epoch  892/30 Cost: 0.250738\n",
      "Epoch  893/30 Cost: 0.250143\n",
      "Epoch  894/30 Cost: 0.249555\n",
      "Epoch  895/30 Cost: 0.248956\n",
      "Epoch  896/30 Cost: 0.248369\n",
      "Epoch  897/30 Cost: 0.247790\n",
      "Epoch  898/30 Cost: 0.247209\n",
      "Epoch  899/30 Cost: 0.246618\n",
      "Epoch  900/30 Cost: 0.246041\n",
      "Epoch  901/30 Cost: 0.245463\n",
      "Epoch  902/30 Cost: 0.244897\n",
      "Epoch  903/30 Cost: 0.244311\n",
      "Epoch  904/30 Cost: 0.243743\n",
      "Epoch  905/30 Cost: 0.243190\n",
      "Epoch  906/30 Cost: 0.242627\n",
      "Epoch  907/30 Cost: 0.242065\n",
      "Epoch  908/30 Cost: 0.241501\n",
      "Epoch  909/30 Cost: 0.240942\n",
      "Epoch  910/30 Cost: 0.240382\n",
      "Epoch  911/30 Cost: 0.239837\n",
      "Epoch  912/30 Cost: 0.239289\n",
      "Epoch  913/30 Cost: 0.238744\n",
      "Epoch  914/30 Cost: 0.238194\n",
      "Epoch  915/30 Cost: 0.237648\n",
      "Epoch  916/30 Cost: 0.237112\n",
      "Epoch  917/30 Cost: 0.236581\n",
      "Epoch  918/30 Cost: 0.236049\n",
      "Epoch  919/30 Cost: 0.235503\n",
      "Epoch  920/30 Cost: 0.234969\n",
      "Epoch  921/30 Cost: 0.234451\n",
      "Epoch  922/30 Cost: 0.233926\n",
      "Epoch  923/30 Cost: 0.233399\n",
      "Epoch  924/30 Cost: 0.232871\n",
      "Epoch  925/30 Cost: 0.232352\n",
      "Epoch  926/30 Cost: 0.231841\n",
      "Epoch  927/30 Cost: 0.231332\n",
      "Epoch  928/30 Cost: 0.230814\n",
      "Epoch  929/30 Cost: 0.230301\n",
      "Epoch  930/30 Cost: 0.229784\n",
      "Epoch  931/30 Cost: 0.229281\n",
      "Epoch  932/30 Cost: 0.228771\n",
      "Epoch  933/30 Cost: 0.228277\n",
      "Epoch  934/30 Cost: 0.227778\n",
      "Epoch  935/30 Cost: 0.227284\n",
      "Epoch  936/30 Cost: 0.226788\n",
      "Epoch  937/30 Cost: 0.226297\n",
      "Epoch  938/30 Cost: 0.225801\n",
      "Epoch  939/30 Cost: 0.225311\n",
      "Epoch  940/30 Cost: 0.224827\n",
      "Epoch  941/30 Cost: 0.224347\n",
      "Epoch  942/30 Cost: 0.223866\n",
      "Epoch  943/30 Cost: 0.223386\n",
      "Epoch  944/30 Cost: 0.222896\n",
      "Epoch  945/30 Cost: 0.222423\n",
      "Epoch  946/30 Cost: 0.221946\n",
      "Epoch  947/30 Cost: 0.221477\n",
      "Epoch  948/30 Cost: 0.221010\n",
      "Epoch  949/30 Cost: 0.220536\n",
      "Epoch  950/30 Cost: 0.220080\n",
      "Epoch  951/30 Cost: 0.219613\n",
      "Epoch  952/30 Cost: 0.219149\n",
      "Epoch  953/30 Cost: 0.218698\n",
      "Epoch  954/30 Cost: 0.218239\n",
      "Epoch  955/30 Cost: 0.217779\n",
      "Epoch  956/30 Cost: 0.217323\n",
      "Epoch  957/30 Cost: 0.216875\n",
      "Epoch  958/30 Cost: 0.216432\n",
      "Epoch  959/30 Cost: 0.215985\n",
      "Epoch  960/30 Cost: 0.215535\n",
      "Epoch  961/30 Cost: 0.215094\n",
      "Epoch  962/30 Cost: 0.214651\n",
      "Epoch  963/30 Cost: 0.214223\n",
      "Epoch  964/30 Cost: 0.213771\n",
      "Epoch  965/30 Cost: 0.213336\n",
      "Epoch  966/30 Cost: 0.212909\n",
      "Epoch  967/30 Cost: 0.212479\n",
      "Epoch  968/30 Cost: 0.212045\n",
      "Epoch  969/30 Cost: 0.211626\n",
      "Epoch  970/30 Cost: 0.211199\n",
      "Epoch  971/30 Cost: 0.210774\n",
      "Epoch  972/30 Cost: 0.210359\n",
      "Epoch  973/30 Cost: 0.209933\n",
      "Epoch  974/30 Cost: 0.209515\n",
      "Epoch  975/30 Cost: 0.209106\n",
      "Epoch  976/30 Cost: 0.208686\n",
      "Epoch  977/30 Cost: 0.208268\n",
      "Epoch  978/30 Cost: 0.207867\n",
      "Epoch  979/30 Cost: 0.207451\n",
      "Epoch  980/30 Cost: 0.207042\n",
      "Epoch  981/30 Cost: 0.206643\n",
      "Epoch  982/30 Cost: 0.206242\n",
      "Epoch  983/30 Cost: 0.205845\n",
      "Epoch  984/30 Cost: 0.205438\n",
      "Epoch  985/30 Cost: 0.205049\n",
      "Epoch  986/30 Cost: 0.204651\n",
      "Epoch  987/30 Cost: 0.204261\n",
      "Epoch  988/30 Cost: 0.203871\n",
      "Epoch  989/30 Cost: 0.203486\n",
      "Epoch  990/30 Cost: 0.203094\n",
      "Epoch  991/30 Cost: 0.202699\n",
      "Epoch  992/30 Cost: 0.202323\n",
      "Epoch  993/30 Cost: 0.201936\n",
      "Epoch  994/30 Cost: 0.201562\n",
      "Epoch  995/30 Cost: 0.201179\n",
      "Epoch  996/30 Cost: 0.200803\n",
      "Epoch  997/30 Cost: 0.200429\n",
      "Epoch  998/30 Cost: 0.200052\n",
      "Epoch  999/30 Cost: 0.199688\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\t# H(x) 계산\n",
    "\tprediction = model(x_train)\n",
    "\t# model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "\t# cost 계산\n",
    "\tcost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "\t# cost로 H(x) 개선하는 부분\n",
    "\t# gradient를 0으로 초기화\n",
    "\toptimizer.zero_grad()\n",
    "\t# 비용 함수를 미분하여 gradient 계산\n",
    "\tcost.backward()\n",
    "\t# W와 b를 업데이트\n",
    "\toptimizer.step()\n",
    "\n",
    "\t# if epoch % 100 == 0:\n",
    "\t# # 100번마다 로그 출력\n",
    "\tprint('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "\t\tepoch, nb_epochs, cost.item()\n",
    "\t  ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[ 73.,  66.,  70.],\n",
      "        [ 96.,  98., 100.]]), tensor([[142.],\n",
      "        [196.]])]\n",
      "1 [tensor([[73., 80., 75.],\n",
      "        [93., 88., 93.]]), tensor([[152.],\n",
      "        [185.]])]\n",
      "2 [tensor([[89., 91., 90.]]), tensor([[180.]])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "\tprint(i, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel(\n",
      "  (linear1): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (linear2): Linear(in_features=6, out_features=3, bias=True)\n",
      "  (linear3): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear1 = nn.Linear(3, 6) # bias=False로 설정 가능\n",
    "\t\tself.linear2 = nn.Linear(6, 3)\n",
    "\t\tself.linear3 = nn.Linear(3, 1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\tx = self.linear2(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\tx = self.linear3(x)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Batch 0/3 Loss: 25308.191406\n",
      "Epoch    0/1000 Batch 1/3 Loss: 90.276077\n",
      "Epoch    0/1000 Batch 2/3 Loss: 209751.750000\n",
      "Epoch   50/1000 Batch 0/3 Loss: 10444.890625\n",
      "Epoch   50/1000 Batch 1/3 Loss: 16808.371094\n",
      "Epoch   50/1000 Batch 2/3 Loss: 8214.852539\n",
      "Epoch  100/1000 Batch 0/3 Loss: 5166.576172\n",
      "Epoch  100/1000 Batch 1/3 Loss: 4099.434570\n",
      "Epoch  100/1000 Batch 2/3 Loss: 1662.861450\n",
      "Epoch  150/1000 Batch 0/3 Loss: 1426.677734\n",
      "Epoch  150/1000 Batch 1/3 Loss: 1586.862061\n",
      "Epoch  150/1000 Batch 2/3 Loss: 82.719604\n",
      "Epoch  200/1000 Batch 0/3 Loss: 480.221802\n",
      "Epoch  200/1000 Batch 1/3 Loss: 240.704498\n",
      "Epoch  200/1000 Batch 2/3 Loss: 1342.057983\n",
      "Epoch  250/1000 Batch 0/3 Loss: 396.086304\n",
      "Epoch  250/1000 Batch 1/3 Loss: 584.896484\n",
      "Epoch  250/1000 Batch 2/3 Loss: 224.670532\n",
      "Epoch  300/1000 Batch 0/3 Loss: 273.377747\n",
      "Epoch  300/1000 Batch 1/3 Loss: 434.462006\n",
      "Epoch  300/1000 Batch 2/3 Loss: 698.428345\n",
      "Epoch  350/1000 Batch 0/3 Loss: 456.259857\n",
      "Epoch  350/1000 Batch 1/3 Loss: 277.386597\n",
      "Epoch  350/1000 Batch 2/3 Loss: 636.551941\n",
      "Epoch  400/1000 Batch 0/3 Loss: 492.977112\n",
      "Epoch  400/1000 Batch 1/3 Loss: 138.051514\n",
      "Epoch  400/1000 Batch 2/3 Loss: 843.620789\n",
      "Epoch  450/1000 Batch 0/3 Loss: 574.769958\n",
      "Epoch  450/1000 Batch 1/3 Loss: 373.192200\n",
      "Epoch  450/1000 Batch 2/3 Loss: 212.941910\n",
      "Epoch  500/1000 Batch 0/3 Loss: 603.601562\n",
      "Epoch  500/1000 Batch 1/3 Loss: 137.744781\n",
      "Epoch  500/1000 Batch 2/3 Loss: 623.628235\n",
      "Epoch  550/1000 Batch 0/3 Loss: 226.987946\n",
      "Epoch  550/1000 Batch 1/3 Loss: 389.383057\n",
      "Epoch  550/1000 Batch 2/3 Loss: 873.676208\n",
      "Epoch  600/1000 Batch 0/3 Loss: 625.858765\n",
      "Epoch  600/1000 Batch 1/3 Loss: 336.880310\n",
      "Epoch  600/1000 Batch 2/3 Loss: 182.909363\n",
      "Epoch  650/1000 Batch 0/3 Loss: 732.926880\n",
      "Epoch  650/1000 Batch 1/3 Loss: 278.348358\n",
      "Epoch  650/1000 Batch 2/3 Loss: 81.815308\n",
      "Epoch  700/1000 Batch 0/3 Loss: 439.707428\n",
      "Epoch  700/1000 Batch 1/3 Loss: 501.477966\n",
      "Epoch  700/1000 Batch 2/3 Loss: 231.682816\n",
      "Epoch  750/1000 Batch 0/3 Loss: 585.594666\n",
      "Epoch  750/1000 Batch 1/3 Loss: 146.462616\n",
      "Epoch  750/1000 Batch 2/3 Loss: 641.973877\n",
      "Epoch  800/1000 Batch 0/3 Loss: 145.686035\n",
      "Epoch  800/1000 Batch 1/3 Loss: 587.500916\n",
      "Epoch  800/1000 Batch 2/3 Loss: 640.231445\n",
      "Epoch  850/1000 Batch 0/3 Loss: 115.513031\n",
      "Epoch  850/1000 Batch 1/3 Loss: 651.298462\n",
      "Epoch  850/1000 Batch 2/3 Loss: 577.955750\n",
      "Epoch  900/1000 Batch 0/3 Loss: 550.446655\n",
      "Epoch  900/1000 Batch 1/3 Loss: 164.635468\n",
      "Epoch  900/1000 Batch 2/3 Loss: 679.482178\n",
      "Epoch  950/1000 Batch 0/3 Loss: 157.940475\n",
      "Epoch  950/1000 Batch 1/3 Loss: 498.500854\n",
      "Epoch  950/1000 Batch 2/3 Loss: 795.658997\n",
      "Epoch 1000/1000 Batch 0/3 Loss: 731.141357\n",
      "Epoch 1000/1000 Batch 1/3 Loss: 150.959122\n",
      "Epoch 1000/1000 Batch 2/3 Loss: 342.373657\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-1)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\tfor idx, batch in enumerate(dataloader):\n",
    "\t\tx, y_true = batch\n",
    "\n",
    "\t\ty_pred = model(x)\n",
    "\n",
    "\t\tloss = F.mse_loss(y_pred, y_true)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif epoch % 50 == 0:\n",
    "\t\t\tprint('Epoch {:4d}/{} Batch {}/{} Loss: {:.6f}'.format(epoch, epochs, idx, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class CustomDataset(Dataset): \n",
    "  def __init__(self):\n",
    "    self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "    self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "  def __getitem__(self, idx): \n",
    "    x = torch.FloatTensor(self.x_data[idx])\n",
    "    y = torch.FloatTensor(self.y_data[idx])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([73., 80., 75.]), tensor([152.]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/0 Batch 3/1000 Loss: 35371.648438\n",
      "Epoch    0/1 Batch 3/1000 Loss: 3645.324219\n",
      "Epoch    0/2 Batch 3/1000 Loss: 39348.425781\n",
      "Epoch   50/0 Batch 3/1000 Loss: 7674.965820\n",
      "Epoch   50/1 Batch 3/1000 Loss: 5383.468750\n",
      "Epoch   50/2 Batch 3/1000 Loss: 8824.833984\n",
      "Epoch  100/0 Batch 3/1000 Loss: 1884.051514\n",
      "Epoch  100/1 Batch 3/1000 Loss: 2352.415283\n",
      "Epoch  100/2 Batch 3/1000 Loss: 896.020935\n",
      "Epoch  150/0 Batch 3/1000 Loss: 12562.760742\n",
      "Epoch  150/1 Batch 3/1000 Loss: 10898.626953\n",
      "Epoch  150/2 Batch 3/1000 Loss: 5730.796875\n",
      "Epoch  200/0 Batch 3/1000 Loss: 2221.288330\n",
      "Epoch  200/1 Batch 3/1000 Loss: 4320.256836\n",
      "Epoch  200/2 Batch 3/1000 Loss: 5819.720703\n",
      "Epoch  250/0 Batch 3/1000 Loss: 2690.535156\n",
      "Epoch  250/1 Batch 3/1000 Loss: 3419.103516\n",
      "Epoch  250/2 Batch 3/1000 Loss: 814.977661\n",
      "Epoch  300/0 Batch 3/1000 Loss: 14816.242188\n",
      "Epoch  300/1 Batch 3/1000 Loss: 10681.101562\n",
      "Epoch  300/2 Batch 3/1000 Loss: 5712.868164\n",
      "Epoch  350/0 Batch 3/1000 Loss: 2487.576660\n",
      "Epoch  350/1 Batch 3/1000 Loss: 3901.520752\n",
      "Epoch  350/2 Batch 3/1000 Loss: 4733.655762\n",
      "Epoch  400/0 Batch 3/1000 Loss: 1016.498047\n",
      "Epoch  400/1 Batch 3/1000 Loss: 3393.369629\n",
      "Epoch  400/2 Batch 3/1000 Loss: 6590.780762\n",
      "Epoch  450/0 Batch 3/1000 Loss: 17607.357422\n",
      "Epoch  450/1 Batch 3/1000 Loss: 9444.738281\n",
      "Epoch  450/2 Batch 3/1000 Loss: 7005.687012\n",
      "Epoch  500/0 Batch 3/1000 Loss: 4452.958496\n",
      "Epoch  500/1 Batch 3/1000 Loss: 4211.116211\n",
      "Epoch  500/2 Batch 3/1000 Loss: 1909.932495\n",
      "Epoch  550/0 Batch 3/1000 Loss: 2519.494873\n",
      "Epoch  550/1 Batch 3/1000 Loss: 2006.918091\n",
      "Epoch  550/2 Batch 3/1000 Loss: 5811.400879\n",
      "Epoch  600/0 Batch 3/1000 Loss: 17572.167969\n",
      "Epoch  600/1 Batch 3/1000 Loss: 7954.222656\n",
      "Epoch  600/2 Batch 3/1000 Loss: 14918.537109\n",
      "Epoch  650/0 Batch 3/1000 Loss: 4462.377441\n",
      "Epoch  650/1 Batch 3/1000 Loss: 7984.449219\n",
      "Epoch  650/2 Batch 3/1000 Loss: 1604.147583\n",
      "Epoch  700/0 Batch 3/1000 Loss: 797.936096\n",
      "Epoch  700/1 Batch 3/1000 Loss: 4165.145508\n",
      "Epoch  700/2 Batch 3/1000 Loss: 257.576141\n",
      "Epoch  750/0 Batch 3/1000 Loss: 15664.171875\n",
      "Epoch  750/1 Batch 3/1000 Loss: 12785.347656\n",
      "Epoch  750/2 Batch 3/1000 Loss: 10022.460938\n",
      "Epoch  800/0 Batch 3/1000 Loss: 8432.103516\n",
      "Epoch  800/1 Batch 3/1000 Loss: 5004.745605\n",
      "Epoch  800/2 Batch 3/1000 Loss: 13396.904297\n",
      "Epoch  850/0 Batch 3/1000 Loss: 404.293396\n",
      "Epoch  850/1 Batch 3/1000 Loss: 1059.820190\n",
      "Epoch  850/2 Batch 3/1000 Loss: 1221.044800\n",
      "Epoch  900/0 Batch 3/1000 Loss: 10005.833984\n",
      "Epoch  900/1 Batch 3/1000 Loss: 12929.373047\n",
      "Epoch  900/2 Batch 3/1000 Loss: 15105.775391\n",
      "Epoch  950/0 Batch 3/1000 Loss: 11445.268555\n",
      "Epoch  950/1 Batch 3/1000 Loss: 15645.207031\n",
      "Epoch  950/2 Batch 3/1000 Loss: 6153.154785\n",
      "Epoch 1000/0 Batch 3/1000 Loss: 952.101074\n",
      "Epoch 1000/1 Batch 3/1000 Loss: 482.707336\n",
      "Epoch 1000/2 Batch 3/1000 Loss: 60.143650\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-1)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\tfor idx, batch in enumerate(dataloader):\n",
    "\t\tx, y_true = batch\n",
    "\n",
    "\t\ty_pred = model(x)\n",
    "\n",
    "\t\tloss = F.mse_loss(y_pred, y_true)\n",
    "\n",
    "\t\toptimzizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif epoch % 50 == 0:\n",
    "\t\t\tprint('Epoch {:4d}/{} Batch {}/{} Loss: {:.6f}'.format(epoch, idx, len(dataloader), epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Batch 1/3 Loss: 28020.972656\n",
      "Epoch    0/1000 Batch 2/3 Loss: 64575.546875\n",
      "Epoch    0/1000 Batch 3/3 Loss: 6940.395508\n",
      "Epoch   50/1000 Batch 1/3 Loss: 4183.465820\n",
      "Epoch   50/1000 Batch 2/3 Loss: 7583.054688\n",
      "Epoch   50/1000 Batch 3/3 Loss: 1418.866089\n",
      "Epoch  100/1000 Batch 1/3 Loss: 514.882690\n",
      "Epoch  100/1000 Batch 2/3 Loss: 2072.714355\n",
      "Epoch  100/1000 Batch 3/3 Loss: 2944.064209\n",
      "Epoch  150/1000 Batch 1/3 Loss: 8760.905273\n",
      "Epoch  150/1000 Batch 2/3 Loss: 10963.647461\n",
      "Epoch  150/1000 Batch 3/3 Loss: 18136.341797\n",
      "Epoch  200/1000 Batch 1/3 Loss: 10078.388672\n",
      "Epoch  200/1000 Batch 2/3 Loss: 12278.636719\n",
      "Epoch  200/1000 Batch 3/3 Loss: 19695.853516\n",
      "Epoch  250/1000 Batch 1/3 Loss: 2080.991699\n",
      "Epoch  250/1000 Batch 2/3 Loss: 1724.023071\n",
      "Epoch  250/1000 Batch 3/3 Loss: 376.734619\n",
      "Epoch  300/1000 Batch 1/3 Loss: 5707.285645\n",
      "Epoch  300/1000 Batch 2/3 Loss: 1671.268433\n",
      "Epoch  300/1000 Batch 3/3 Loss: 8125.472656\n",
      "Epoch  350/1000 Batch 1/3 Loss: 14163.880859\n",
      "Epoch  350/1000 Batch 2/3 Loss: 12570.291992\n",
      "Epoch  350/1000 Batch 3/3 Loss: 21206.560547\n",
      "Epoch  400/1000 Batch 1/3 Loss: 8674.212891\n",
      "Epoch  400/1000 Batch 2/3 Loss: 7363.761230\n",
      "Epoch  400/1000 Batch 3/3 Loss: 13971.102539\n",
      "Epoch  450/1000 Batch 1/3 Loss: 322.590271\n",
      "Epoch  450/1000 Batch 2/3 Loss: 532.944153\n",
      "Epoch  450/1000 Batch 3/3 Loss: 520.892700\n",
      "Epoch  500/1000 Batch 1/3 Loss: 8919.812500\n",
      "Epoch  500/1000 Batch 2/3 Loss: 11451.015625\n",
      "Epoch  500/1000 Batch 3/3 Loss: 6686.146484\n",
      "Epoch  550/1000 Batch 1/3 Loss: 16877.146484\n",
      "Epoch  550/1000 Batch 2/3 Loss: 12004.371094\n",
      "Epoch  550/1000 Batch 3/3 Loss: 24080.753906\n",
      "Epoch  600/1000 Batch 1/3 Loss: 5504.486328\n",
      "Epoch  600/1000 Batch 2/3 Loss: 5519.948242\n",
      "Epoch  600/1000 Batch 3/3 Loss: 2763.785400\n",
      "Epoch  650/1000 Batch 1/3 Loss: 2522.686035\n",
      "Epoch  650/1000 Batch 2/3 Loss: 2277.714355\n",
      "Epoch  650/1000 Batch 3/3 Loss: 121.860229\n",
      "Epoch  700/1000 Batch 1/3 Loss: 16828.359375\n",
      "Epoch  700/1000 Batch 2/3 Loss: 8958.855469\n",
      "Epoch  700/1000 Batch 3/3 Loss: 20688.425781\n",
      "Epoch  750/1000 Batch 1/3 Loss: 17793.980469\n",
      "Epoch  750/1000 Batch 2/3 Loss: 9545.635742\n",
      "Epoch  750/1000 Batch 3/3 Loss: 21395.705078\n",
      "Epoch  800/1000 Batch 1/3 Loss: 1625.689575\n",
      "Epoch  800/1000 Batch 2/3 Loss: 2066.032715\n",
      "Epoch  800/1000 Batch 3/3 Loss: 2194.069580\n",
      "Epoch  850/1000 Batch 1/3 Loss: 2325.767578\n",
      "Epoch  850/1000 Batch 2/3 Loss: 8495.506836\n",
      "Epoch  850/1000 Batch 3/3 Loss: 3558.968262\n",
      "Epoch  900/1000 Batch 1/3 Loss: 14369.747070\n",
      "Epoch  900/1000 Batch 2/3 Loss: 24213.792969\n",
      "Epoch  900/1000 Batch 3/3 Loss: 11385.576172\n",
      "Epoch  950/1000 Batch 1/3 Loss: 18557.625000\n",
      "Epoch  950/1000 Batch 2/3 Loss: 9031.178711\n",
      "Epoch  950/1000 Batch 3/3 Loss: 9457.959961\n",
      "Epoch 1000/1000 Batch 1/3 Loss: 620.540222\n",
      "Epoch 1000/1000 Batch 2/3 Loss: 1032.268677\n",
      "Epoch 1000/1000 Batch 3/3 Loss: 32.109440\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-1)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\tfor idx, batch in enumerate(dataloader):\n",
    "\t\tx, y_true = batch\n",
    "\n",
    "\t\ty_pred = model(x)\n",
    "\n",
    "\t\tloss = F.mse_loss(y_pred, y_true)\n",
    "\n",
    "\t\t\n",
    "\t\tloss.backward()\n",
    "\t\toptimzizer.zero_grad()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif epoch % 50 == 0:\n",
    "\t\t\tprint('Epoch {:4d}/{} Batch {}/{} Loss: {:.6f}'.format(epoch, epochs, idx+1, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
