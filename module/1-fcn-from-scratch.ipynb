{"cells":[{"cell_type":"markdown","metadata":{},"source":["# FCN 논문 구현 및 Segmentation 실습"]},{"cell_type":"markdown","metadata":{},"source":["## Prep"]},{"cell_type":"markdown","metadata":{},"source":["### 라이브러리"]},{"cell_type":"code","execution_count":1,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from glob import glob\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import torchvision\n","from torchvision import models\n","from torchvision import transforms\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["### 데이터 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_list = glob('/kaggle/input/flood-area-segmentation/Image/*')\n","\n","mask_list = glob('/kaggle/input/flood-area-segmentation/Mask/*')\n","len(image_list), len(mask_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n","axes = axes.flatten()\n","\n","for i in range(4):\n","    img_path = image_list[i]\n","    mask_path = image_list[i].replace('Image', 'Mask').replace('jpg', 'png')\n","    \n","    axes[2 * i].imshow(Image.open(img_path))\n","    axes[2 * i].set_title(f'Image {i+1}')\n","    axes[2 * i].axis('off')\n","    \n","    axes[2 * i + 1].imshow(Image.open(mask_path))\n","    axes[2 * i + 1].set_title(f'Mask {i+1}')\n","    axes[2 * i + 1].axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n","axes = axes.flatten()\n","for i, ax in enumerate(axes):\n","    img_path = image_list[i]\n","    mask_path = image_list[i].replace('Image', 'Mask').replace('jpg', 'png')\n","    img = Image.open(img_path)\n","    mask = Image.open(mask_path)\n","    ax.imshow(img)\n","    ax.imshow(mask, alpha=0.5)\n","    ax.axis('off')"]},{"cell_type":"markdown","metadata":{},"source":["### 마스크 Binary화 필요"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["unique, counts = np.unique(mask_list, return_counts=True)\n","for mask_path in mask_list:\n","    mask = Image.open(mask_path)\n","    mask = np.array(mask)\n","    print(mask)\n","#     print(mask.shape)\n","    print(np.unique(mask))\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["### 이미지 데이터 차원 통일"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["exclude = []\n","\n","for img_path in image_list:\n","    img = Image.open(img_path)\n","    img = np.array(img)\n","    if img.ndim != 3 or img.shape[2] != 3:\n","        print(img.shape)\n","        print(img.ndim)\n","        print(img_path)\n","        exclude.append(img_path)"]},{"cell_type":"markdown","metadata":{},"source":["### 차원 불일치 데이터 제거"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for ex in exclude:\n","    image_list.remove(ex)\n","    mask_list.remove(ex.replace('Image', 'Mask').replace('jpg', 'png'))"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class FloodDataset(Dataset):\n","    def __init__(self, image_list, mask_list, transform):\n","        \n","        self.image_list = image_list\n","        self.mask_list = mask_list\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        \n","        return len(self.image_list)\n","    \n","    def __getitem__(self, idx):\n","        \n","        image_path = self.image_list[idx]\n","        mask_path = image_path.replace('Image', 'Mask').replace('jpg', 'png')\n","        \n","        image = Image.open(image_path)\n","        mask = Image.open(mask_path)\n","        \n","        image = self.transform(image)\n","        mask = self.transform(mask)\n","        \n","        mask = (mask > 0.5).float()\n","        \n","        return image, mask"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor(),\n","                               transforms.Resize((224, 224)),\n","                               ])"]},{"cell_type":"markdown","metadata":{},"source":["### DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["flood_dataset = FloodDataset(image_list, mask_list, transform = transform)\n","print(flood_dataset.__len__())\n","\n","train_size = int(0.8 * len(flood_dataset))\n","val_size = len(flood_dataset) - train_size\n","\n","train_ds, val_ds = random_split(flood_dataset, [train_size, val_size])\n","print(f\"Train dataset size: {len(train_ds)} | Validation dataset size: {len(val_ds)}\")\n","\n","train_dl = DataLoader(train_ds, batch_size = 16, shuffle = True)\n","val_dl = DataLoader(val_ds, batch_size = 16, shuffle = False)\n","\n","imgs, masks = next(iter(train_dl))\n","print(imgs.shape, masks.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 학습 및 추론\n","\n","\n","### 모델 구현\n","아래 그림과 조건에 맞게 모델을 구현해봅니다.\n","![](https://velog.velcdn.com/images%2Fleejaejun%2Fpost%2Fc3b69a0d-2329-4903-9baa-de2949af87fb%2Fimage.png)\n","\n","- VGG16 Backbone을 활용\n","    - VGG16 Backbone은 총 5개의 Conv block으로 구성\n","    - 이미지를 Backbone에 통과\n","    - 추가 Conv layer 2개를 통과\n","    - 마지막 Feature map Channel size = class num과 같아야 함\n","- Skip connection\n","    - 3, 4번 block을 통과한 Feature map을 재사용\n","    - Upsampling Feature map과 더하기 위하여 Channel size 조정 필요\n","    - 결합 방식: tensor_1 + tensor_2(더하기 연산)\n","- Upsampling\n","    - 두 개의 Upsampling 레이어를 통과\n","    - 총 세 번 Upsampling(2배 증가 2번, 8배 증가 1번)\n","    - 첫 번째 2배 증가 후 4번 Block과 결합\n","    - 두 번째 2배 증가 후 3번 Block과 결합"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["class FCN8s(nn.Module):\n","    def __init__(self, n_classes):\n","        super(FCN8s, self).__init__()\n","        vgg = models.vgg16(pretrained=True)\n","        features = list(vgg.features.children())\n","\n","        # VGG16의 각 블록을 PyTorch Sequential로 구성\n","        self.block3 = nn.Sequential(*features[:17])  # Conv1 ~ Conv3\n","        self.block4 = nn.Sequential(*features[17:24])  # Conv4\n","        self.block5 = nn.Sequential(*features[24:])  # Conv5\n","\n","        # 추가 Conv 레이어\n","        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7, padding=3)\n","        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n","\n","        # FCN에서 사용할 1x1 Conv\n","        self.conv1x1_pool3 = nn.Conv2d(256, n_classes, kernel_size=1)\n","        self.conv1x1_pool4 = nn.Conv2d(512, n_classes, kernel_size=1)\n","        self.conv1x1_output = nn.Conv2d(4096, n_classes, kernel_size=1)\n","\n","        # Transposed convolutions for upsampling\n","        self.upconv2 = nn.ConvTranspose2d(n_classes, n_classes, kernel_size=4, stride=2, padding=1)\n","        self.upconv8 = nn.ConvTranspose2d(n_classes, n_classes, kernel_size=8, stride=8, padding=0)\n","\n","    def forward(self, x):\n","        p3 = self.block3(x) # (3, 256, 256) -> (256, 32, 32) # H/8\n","        p4 = self.block4(p3) # (256, 32, 32) -> (512, 16, 16) # H/16 \n","        p5 = self.block5(p4) # (512, 16, 16) -> (512, 8, 8)\n","        p5 = F.relu(self.conv6(p5)) # (512, 8, 8) -> (4096, 8, 8)\n","        p5 = F.relu(self.conv7(p5)) # (4096, 8, 8) -> (4096, 8, 8)\n","\n","\n","        # Decoder\n","        output = self.conv1x1_output(p5)\n","        output = self.upconv2(output) + self.conv1x1_pool4(p4)\n","        output = self.upconv2(output) + self.conv1x1_pool3(p3)\n","        output = self.upconv8(output)\n","\n","\n","\n","    # def forward(self, x):\n","    #     x = self.block3(x)\n","    #     fmap1 = self.conv1x1_pool3(x)\n","    #     x = self.block4(x)\n","    #     fmap2 = self.conv1x1_pool4(x)\n","    #     x = self.block5(x)\n","\n","    #     x = self.conv6(x)\n","    #     x = self.conv7(x)\n","    #     x = self.conv1x1_output(x)\n","\n","    #     x = self.upconv2(x)\n","    #     x = x + fmap2\n","    #     x = self.upconv2(x)\n","    #     x = x + fmap1\n","    #     output = self.upconv8(x)\n","\n","        return output"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/vin_ah/miniconda3/envs/module0/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/Users/vin_ah/miniconda3/envs/module0/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 1, 224, 224])\n"]}],"source":["# 모델 인스턴스 생성\n","model = FCN8s(n_classes=1).to(device)\n","\n","input_image = torch.randn(1, 3, 224, 224).to(device)\n","\n","output = model(input_image)\n","print(output.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### 학습 루프 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["criterion = nn.BCEWithLogitsLoss()\n","optim = torch.optim.Adam(params = model.parameters(), lr = 5e-4)\n","epochs = 20"]},{"cell_type":"markdown","metadata":{},"source":["### 평가 지표"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def IoU(output, mask):\n","    \"\"\"\n","    1. pred를 threshold 기준 Binary 분류\n","    2. pred과 gt 사이 교집합 영역(픽셀) 계산\n","    3. pred와 gt의 합집합 영역 계산\n","    4. zero division 방지\n","    \"\"\"\n","    threshold = 0.5\n","    output = (output > threshold).float()\n","    intersection = torch.sum(output * mask)\n","    union = torch.sum(output) + torch.sum(mask) - intersection\n","    \n","    return intersection / union + 1e-7\n","    \n","    \n","    \n","def PA(output, mask):\n","    \"\"\"\n","    1. pred를 threshold 기준 Binary 분류\n","    2. pred과 gt 일치 여부 계산\n","    3. 전체 픽셀 중 일치 픽셀 수 반환\n","    \"\"\"\n","    \n","    threshold = 0.5\n","    output = (output > threshold).float()\n","    correct = torch.sum(output==mask) # TP, TN\n","    total = torch.numel(output)\n","    \n","    return correct / total"]},{"cell_type":"markdown","metadata":{},"source":["### 학습 루프 함수"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_and_validate(model, train_loader, val_loader, optim, criterion, epochs):\n","    train_losses = []\n","    train_IoUs = []\n","    train_PAs = []\n","    val_losses = []\n","    val_IoUs = []\n","    val_PAs = []\n","    \n","    for epoch in range(epochs):\n","        train_loss = 0\n","        train_IoU = 0\n","        train_PA = 0\n","        model.train()\n","        for img, mask in tqdm(train_loader):\n","            # YOUR CODE\n","        \n","        val_loss = 0\n","        val_IoU = 0\n","        val_PA = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for img, mask in tqdm(val_loader):\n","                # YOUR CODE\n","            \n","    return train_losses, train_IoUs, train_PAs, val_losses, val_IoUs, val_PAs\n"]},{"cell_type":"markdown","metadata":{},"source":["### 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_losses, train_IoUs, train_PAs, val_losses, val_IoUs, val_PAs = train_and_validate(model, train_dl, val_dl, optim, criterion, epochs)"]},{"cell_type":"markdown","metadata":{},"source":["### 로그 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(train_losses, label = 'train loss')\n","plt.plot(val_losses, label = 'val loss')\n","plt.legend()\n","plt.show()\n","plt.plot(train_IoUs, label = 'train IoU')\n","plt.plot(val_IoUs, label = 'val IoU')\n","plt.legend()\n","plt.show()\n","plt.plot(train_PA, label = 'train Pixel Accuracy')\n","plt.plot(val_PA, label = 'val Pixel Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_batch(model, data_loader):\n","    model.eval()\n","    with torch.no_grad():\n","        for img, mask in tqdm(data_loader):\n","            img = img.to(device)\n","            output = model(img)\n","            output = (output > 0.5).float()\n","            img = img.cpu().numpy().transpose(0, 2, 3, 1)\n","            mask = mask.cpu().numpy().transpose(0, 2, 3, 1)\n","            output = output.cpu().numpy().transpose(0, 2, 3, 1)\n","            break\n","    for i in range(data_loader.batch_size):\n","        fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n","        ax[0].imshow(img[i])\n","        ax[0].set_title('image')\n","        ax[1].imshow(mask[i], cmap = 'gray')\n","        ax[1].set_title('mask')\n","        ax[2].imshow(output[i], cmap = 'gray')\n","        ax[2].set_title('predicted mask')\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 결과 비교"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_batch(model, val_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2738852,"sourceId":4732950,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
